{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "89faad26-a805-4ba6-a0dc-1c0058361777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5970c651-b5dd-4de0-95e3-963d0ca536a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Exercise Data Pipeline with PySpark Week 6\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5489b9bc-664a-4af8-8bde-62015f07a610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pyspark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Exercise Data Pipeline with PySpark Week 6</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1d852d4690>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "51a34e90-8cd4-4c96-a709-1df5f5d1778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " TransactionID           | T642232     \n",
      " CustomerID              | C1010028    \n",
      " CustomerDOB             | 25/8/88     \n",
      " CustGender              | F           \n",
      " CustLocation            | DELHI       \n",
      " CustAccountBalance      | 296828.37   \n",
      " TransactionDate         | 29/8/16     \n",
      " TransactionTime         | 95212       \n",
      " TransactionAmount (INR) | 557         \n",
      "-RECORD 1------------------------------\n",
      " TransactionID           | T87414      \n",
      " CustomerID              | C1010035    \n",
      " CustomerDOB             | 2/3/92      \n",
      " CustGender              | M           \n",
      " CustLocation            | MUMBAI      \n",
      " CustAccountBalance      | 7284.42     \n",
      " TransactionDate         | 1/8/16      \n",
      " TransactionTime         | 111917      \n",
      " TransactionAmount (INR) | 50          \n",
      "-RECORD 2------------------------------\n",
      " TransactionID           | T560676     \n",
      " CustomerID              | C1010035_2  \n",
      " CustomerDOB             | 9/6/80      \n",
      " CustGender              | M           \n",
      " CustLocation            | NAVI MUMBAI \n",
      " CustAccountBalance      | 378013.09   \n",
      " TransactionDate         | 27/8/16     \n",
      " TransactionTime         | 185011      \n",
      " TransactionAmount (INR) | 700         \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transaction = spark.read.csv(\"data/new_bank_transaction.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "\n",
    "df_transaction.show(3, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311f74c-c28e-42b3-9bac-9f0f32341a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"data/new_bank_transaction.csv/\"\n",
    "\n",
    "df_ratings = spark.read.csv(directory + \"part-*.csv\", header=True)\n",
    "\n",
    "df_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa47860c-d92c-4dca-8e25-5a1126d88cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init vars\n",
    "DB_URL = \"jdbc:postgresql://source_db:5432/source\"\n",
    "DB_TABLE = \"marketing_campaign_deposit\" \n",
    "DB_USER = \"postgres\"\n",
    "DB_PASS = \"postgres\"\n",
    "\n",
    "# set config\n",
    "jdbc_url = DB_URL\n",
    "table_name = DB_TABLE\n",
    "connection_properties = {\n",
    "    \"user\": DB_USER,\n",
    "    \"password\": DB_PASS,\n",
    "    \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9baeafa4-4ebe-4e72-ad05-0c42f8d783b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marketing = spark \\\n",
    "              .read \\\n",
    "              .jdbc(url = jdbc_url,\n",
    "                    table = table_name,\n",
    "                    properties = connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdf48b53-4a0d-469a-b4c3-1a461a996a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------\n",
      " loan_data_id       | 1                          \n",
      " age                | 58                         \n",
      " job                | management                 \n",
      " marital_id         | 1                          \n",
      " education_id       | 1                          \n",
      " default            | false                      \n",
      " balance            | $2143                      \n",
      " housing            | true                       \n",
      " loan               | false                      \n",
      " contact            | unknown                    \n",
      " day                | 5                          \n",
      " month              | may                        \n",
      " duration           | 261                        \n",
      " campaign           | 1                          \n",
      " pdays              | -1                         \n",
      " previous           | 0                          \n",
      " poutcome           | unknown                    \n",
      " subscribed_deposit | false                      \n",
      " created_at         | 2025-02-28 15:59:11.102813 \n",
      " updated_at         | 2025-02-28 15:59:11.102813 \n",
      "-RECORD 1----------------------------------------\n",
      " loan_data_id       | 2                          \n",
      " age                | 44                         \n",
      " job                | technician                 \n",
      " marital_id         | 2                          \n",
      " education_id       | 2                          \n",
      " default            | false                      \n",
      " balance            | $29                        \n",
      " housing            | true                       \n",
      " loan               | false                      \n",
      " contact            | unknown                    \n",
      " day                | 5                          \n",
      " month              | may                        \n",
      " duration           | 151                        \n",
      " campaign           | 1                          \n",
      " pdays              | -1                         \n",
      " previous           | 0                          \n",
      " poutcome           | unknown                    \n",
      " subscribed_deposit | false                      \n",
      " created_at         | 2025-02-28 15:59:11.102813 \n",
      " updated_at         | 2025-02-28 15:59:11.102813 \n",
      "-RECORD 2----------------------------------------\n",
      " loan_data_id       | 3                          \n",
      " age                | 33                         \n",
      " job                | entrepreneur               \n",
      " marital_id         | 1                          \n",
      " education_id       | 2                          \n",
      " default            | false                      \n",
      " balance            | $2                         \n",
      " housing            | true                       \n",
      " loan               | true                       \n",
      " contact            | unknown                    \n",
      " day                | 5                          \n",
      " month              | may                        \n",
      " duration           | 76                         \n",
      " campaign           | 1                          \n",
      " pdays              | -1                         \n",
      " previous           | 0                          \n",
      " poutcome           | unknown                    \n",
      " subscribed_deposit | false                      \n",
      " created_at         | 2025-02-28 15:59:11.102813 \n",
      " updated_at         | 2025-02-28 15:59:11.102813 \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_marketing.show(3, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "355d472f-cb3a-45c3-99d1-6a07969be19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_education = spark.read.jdbc(url=jdbc_url, table=\"education_status\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b85a7c5-2fb8-47e9-aa97-eb839650da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- education_id: integer (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_education.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34fa82e9-4bce-458b-bb9f-3d5884741762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " education_id | 1                          \n",
      " value        | tertiary                   \n",
      " created_at   | 2025-02-28 15:31:04.358235 \n",
      " updated_at   | 2025-02-28 15:31:04.358235 \n",
      "-RECORD 1----------------------------------\n",
      " education_id | 2                          \n",
      " value        | secondary                  \n",
      " created_at   | 2025-02-28 15:31:04.358235 \n",
      " updated_at   | 2025-02-28 15:31:04.358235 \n",
      "-RECORD 2----------------------------------\n",
      " education_id | 3                          \n",
      " value        | unknown                    \n",
      " created_at   | 2025-02-28 15:31:04.358235 \n",
      " updated_at   | 2025-02-28 15:31:04.358235 \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_education.show(3, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b7b5387-0d8c-47fd-850f-f4a1927aeaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marital = spark.read.jdbc(url=jdbc_url, table=\"marital_status\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2030f6f8-2764-4839-af6a-649f1451dc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marital_id: integer (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_marital.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c13c79a3-a375-4ed9-87bd-8559be70ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " marital_id | 1                          \n",
      " value      | married                    \n",
      " created_at | 2025-02-28 15:31:01.502136 \n",
      " updated_at | 2025-02-28 15:31:01.502136 \n",
      "-RECORD 1--------------------------------\n",
      " marital_id | 2                          \n",
      " value      | single                     \n",
      " created_at | 2025-02-28 15:31:01.502136 \n",
      " updated_at | 2025-02-28 15:31:01.502136 \n",
      "-RECORD 2--------------------------------\n",
      " marital_id | 3                          \n",
      " value      | divorced                   \n",
      " created_at | 2025-02-28 15:31:01.502136 \n",
      " updated_at | 2025-02-28 15:31:01.502136 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_marital.show(3, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb4ebe-da24-43d1-93e8-4767f0030df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "07abbb1a-c641-4d71-8d54-53d014f71d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " loan_data_id               | 0   \n",
      " age                        | 0   \n",
      " job                        | 0   \n",
      " marital_id                 | 0   \n",
      " education_id               | 0   \n",
      " default                    | 0   \n",
      " balance                    | 0   \n",
      " housing                    | 0   \n",
      " loan                       | 0   \n",
      " contact                    | 0   \n",
      " day                        | 0   \n",
      " month                      | 0   \n",
      " duration                   | 0   \n",
      " campaign                   | 0   \n",
      " days_since_last_campaign   | 0   \n",
      " previous_campaign_contacts | 0   \n",
      " previous_campaign_outcome  | 0   \n",
      " subscribed_deposit         | 0   \n",
      " created_at                 | 0   \n",
      " updated_at                 | 0   \n",
      " duration_in_year           | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store column expressions\n",
    "null_counts = []\n",
    "\n",
    "# Loop to iterate over each column in df_marketing\n",
    "for c in df_marketing.columns:\n",
    "    # Count the number of NULL values for column c\n",
    "    null_count_expr = F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "    \n",
    "    # Add the expression to the null_counts list\n",
    "    null_counts.append(null_count_expr)\n",
    "\n",
    "# Select the columns where the number of NULL values has been calculated\n",
    "df_null_counts = df_marketing.select(null_counts)\n",
    "\n",
    "# Display the result\n",
    "df_null_counts.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be693ae5-d89c-460b-9017-2fcca067689b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a600c0f0-c9de-49bd-95e4-2b043dc8018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " loan_data_id               | 0   \n",
      " age                        | 0   \n",
      " job                        | 0   \n",
      " marital_id                 | 0   \n",
      " education_id               | 0   \n",
      " default                    | 0   \n",
      " balance                    | 0   \n",
      " housing                    | 0   \n",
      " loan                       | 0   \n",
      " contact                    | 0   \n",
      " day                        | 0   \n",
      " month                      | 0   \n",
      " duration                   | 0   \n",
      " campaign                   | 0   \n",
      " days_since_last_campaign   | 0   \n",
      " previous_campaign_contacts | 0   \n",
      " previous_campaign_outcome  | 0   \n",
      " subscribed_deposit         | 0   \n",
      " created_at                 | 0   \n",
      " updated_at                 | 0   \n",
      " duration_in_year           | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_marketing.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_marketing.columns]).show(truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4277ec1-ef38-40c5-b0ce-8963ac126559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b255bcb6-74d7-4386-9c89-20176e7935b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------\n",
      " education_id | 0   \n",
      " value        | 0   \n",
      " created_at   | 0   \n",
      " updated_at   | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store column expressions\n",
    "null_counts = []\n",
    "\n",
    "# Loop to iterate over each column in df_education\n",
    "for c in df_education.columns:\n",
    "    # Count the number of NULL values for column c\n",
    "    null_count_expr = F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "    \n",
    "    # Add the expression to the null_counts list\n",
    "    null_counts.append(null_count_expr)\n",
    "\n",
    "# Select the columns where the number of NULL values has been calculated\n",
    "df_null_counts = df_education.select(null_counts)\n",
    "\n",
    "# Display the result\n",
    "df_null_counts.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e9a192-b3e3-465f-83e6-09ecad9f4ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f7e6bf2c-7964-497c-830e-09fd5c46b0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------\n",
      " education_id | 0   \n",
      " value        | 0   \n",
      " created_at   | 0   \n",
      " updated_at   | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_education.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_education.columns]).show(truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156d533-ecca-44bf-90cf-7ee1f5069901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "28fe654c-0c7c-4e65-b452-a4e785e00672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " marital_id | 0   \n",
      " value      | 0   \n",
      " created_at | 0   \n",
      " updated_at | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store column expressions\n",
    "null_counts = []\n",
    "\n",
    "# Loop to iterate over each column in df_marital\n",
    "for c in df_marital.columns:\n",
    "    # Count the number of NULL values for column c\n",
    "    null_count_expr = F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "    \n",
    "    # Add the expression to the null_counts list\n",
    "    null_counts.append(null_count_expr)\n",
    "\n",
    "# Select the columns where the number of NULL values has been calculated\n",
    "df_null_counts = df_marital.select(null_counts)\n",
    "\n",
    "# Display the result\n",
    "df_null_counts.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd3cccb-9418-4706-a5d9-526c85705fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d4fb892-ade5-42ec-b192-3bc57a6ae448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " marital_id | 0   \n",
      " value      | 0   \n",
      " created_at | 0   \n",
      " updated_at | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_marital.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_marital.columns]).show(truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a6905-2811-4085-b5a4-db4044708974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b2df4f5b-4b93-4229-a5e9-1074d9919d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------\n",
      " TransactionID           | 0    \n",
      " CustomerID              | 0    \n",
      " CustomerDOB             | 0    \n",
      " CustGender              | 1100 \n",
      " CustLocation            | 151  \n",
      " CustAccountBalance      | 2369 \n",
      " TransactionDate         | 0    \n",
      " TransactionTime         | 0    \n",
      " TransactionAmount (INR) | 0    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store column expressions\n",
    "null_counts = []\n",
    "\n",
    "# Loop to iterate over each column in df_transaction\n",
    "for c in df_transaction.columns:\n",
    "    # Count the number of NULL values for column c\n",
    "    null_count_expr = F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "    \n",
    "    # Add the expression to the null_counts list\n",
    "    null_counts.append(null_count_expr)\n",
    "\n",
    "# Select the columns where the number of NULL values has been calculated\n",
    "df_null_counts = df_transaction.select(null_counts)\n",
    "\n",
    "# Display the result\n",
    "df_null_counts.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2b67e-c13f-4895-b5c0-7af988c5de03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d7138287-9b74-4e9b-846d-5e615ea626ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TransactionID: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- CustomerDOB: string (nullable = true)\n",
      " |-- CustGender: string (nullable = true)\n",
      " |-- CustLocation: string (nullable = true)\n",
      " |-- CustAccountBalance: string (nullable = true)\n",
      " |-- TransactionDate: string (nullable = true)\n",
      " |-- TransactionTime: string (nullable = true)\n",
      " |-- TransactionAmount (INR): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transaction.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2367c-efd2-478e-8460-d998c5eb5f6d",
   "metadata": {},
   "source": [
    "### **2. Source to Target Mapping**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e7d91-5971-4acd-aba7-6f5f0b6e0fc9",
   "metadata": {},
   "source": [
    "\r\n",
    "## Column Mapping\r\n",
    "\r\n",
    "### Education Status\r\n",
    "source : table education_status\r\n",
    "\r\n",
    "target : table education_status\r\n",
    "\r\n",
    "| Source Column   | Target Column   | Transformation                                   |\r\n",
    "|----------------|----------------|---------------------------------------------------------|\r\n",
    "| `education_id` | `education_id` | -                       |\r\n",
    "| `value`        | `value`        | - |\r\n",
    "| `created_at`   | `created_at`   | -           |\r\n",
    "| `updated_at`   | `updated_at`   | -          |\r\n",
    "\r\n",
    "\r\n",
    "### Marital Status\r\n",
    "source : table marital_status\r\n",
    "\r\n",
    "target : table marital_status\r\n",
    "| Source Column   | Target Column   | Transformation |\r\n",
    "|----------------|----------------|---------------|\r\n",
    "| `marital_id`   | `marital_id`   | - |\r\n",
    "| `value`        | `value`        | - |\r\n",
    "| `created_at`   | `created_at`   | - |\r\n",
    "| `updated_at`   | `updated_at`   | - |\r\n",
    "\r\n",
    "\r\n",
    "### Marketing Campaign for Deposit\r\n",
    "source : table marketing_campaign_deposit\r\n",
    "\r\n",
    "target : table marketing_campaign_deposit\r\n",
    "| Source Column              | Target Column                | Transformation                                      |\r\n",
    "|----------------------------|-----------------------------|----------------------------------------------------|\r\n",
    "| `loan_data_id`             | `loan_data_id`              | - |\r\n",
    "| `age`                      | `age`                       | - |\r\n",
    "| `job`                      | `job`                       | - |\r\n",
    "| `marital_id`               | `marital_id`                | - |\r\n",
    "| `education_id`             | `education_id`              | - |\r\n",
    "| `\"default\"`                | `\"default\"`                 | - |\r\n",
    "| `balance`                  | `balance`                   | Remove `$` sign and convert to `INT` |\r\n",
    "| `housing`                  | `housing`                   | - |\r\n",
    "| `loan`                     | `loan`                      | - |\r\n",
    "| `contact`                  | `contact`                   | - |\r\n",
    "| `\"day\"`                    | `\"day\"`                     | - |\r\n",
    "| `\"month\"`                  | `\"month\"`                   | - |\r\n",
    "| `duration`                 | `duration`                  | - |\r\n",
    "| `duration`                 | `duration_in_year`          | duration divide by `365`, round down, and cast to `INT` |\r\n",
    "| `campaign`                 | `campaign`                  | - |\r\n",
    "| `pdays`                    | `days_since_last_campaign`  | Rename column |\r\n",
    "| `previous`                 | `previous_campaign_contacts`| Rename column |\r\n",
    "| `poutcome`                 | `previous_campaign_outcome` | Rename column |\r\n",
    "| `subscribed_deposit`       | `subscribed_deposit`        | - |\r\n",
    "| `created_at`               | `created_at`                | - |\r\n",
    "| `updated_at`               | `updated_at`                | - |\r\n",
    "\r\n",
    "### Customers\r\n",
    "source : file new_bank_transaction.csv\r\n",
    "\r\n",
    "target : table customers\r\n",
    "\r\n",
    "| Source Column          | Target Column      | Transformation                                      |\r\n",
    "|------------------------|-------------------|----------------------------------------------------|\r\n",
    "| `CustomerID`          | `customer_id`      | Rename column |\r\n",
    "| `CustomerDOB`         | `birth_date`       | Convert to `DATE` format (`d/M/yy`), adjust years if > 2025 |\r\n",
    "| `CustGender`          | `gender`           | Rename column; Map `M` → `Male`, `F` → `Female`, others → `Other` |\r\n",
    "| `CustLocation`        | `location`         | Rename column |\r\n",
    "| `CustAccountBalance`  | `account_balance`  | Rename column, cast to decimal number |\r\n",
    "\r\n",
    "### Transactions\r\n",
    "source : file new_bank_transaction.csv\r\n",
    "\r\n",
    "target : table transactions\r\n",
    "\r\n",
    "| Source Column                 | Target Column      | Transformation                                                   |\r\n",
    "|--------------------------------|-------------------|-----------------------------------------------------------------|\r\n",
    "| `TransactionID`               | `transaction_id`  | Rename column |\r\n",
    "| `CustomerID`                  | `customer_id`     | Rename column |\r\n",
    "| `TransactionDate`             | `transaction_date` | Convert to `DATE` format (`d/M/yy`), adjust years if > 2025 |\r\n",
    "| `TransactionTime`             | `transaction_time` | Convert to `HH:MM:SS` format |\r\n",
    "| `TransactionAmount (INR)`     | `transaction_amount` | Rename column, cast to decimal number |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb1a21b-2e69-46cc-ba32-6e8050e1a9c2",
   "metadata": {},
   "source": [
    "### **3. Transform Data**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2cfa50a1-0dbf-4c72-9752-d6ea1a9cd1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def logging_process(log_file=\"script/log/info.log\"):\n",
    "    # Configure logging\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=log_file,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "    return logger\n",
    "\n",
    "\n",
    "def init_spark_session():\n",
    "    spark = SparkSession.builder.appName(\n",
    "        \"Exercise Data Pipeline Week_6\"\n",
    "    ).getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3cee24ee-2601-4ca7-9084-60dc0f1c3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pyspark\n",
    "\n",
    "logging_process()\n",
    "\n",
    "\n",
    "def extract_data(\n",
    "    data_name: str, format_data: str\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to extract movie data in csv or database table\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_name (str): name of data or table of data sources\n",
    "    format_data (str): format data of data sources, currently on csv or db\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df (pyspark.sql.DataFrame): dataframe of data sources\n",
    "    \"\"\"\n",
    "    # create spark session\n",
    "    spark = init_spark_session()\n",
    "\n",
    "    # set variable for database\n",
    "    DB_URL = \"jdbc:postgresql://source_db:5432/source\"\n",
    "    DB_USER = \"postgres\"\n",
    "    DB_PASS = \"postgres\"\n",
    "\n",
    "    # set config\n",
    "    jdbc_url = DB_URL\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if format_data.lower() == \"csv\":\n",
    "            logging.info(f\"===== Start Extracting {data_name} data =====\")\n",
    "\n",
    "            df = spark.read.csv(f\"data/{data_name}.csv\", header=True)\n",
    "\n",
    "            logging.info(f\"===== Finish Extracting {data_name} data =====\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        elif format_data.lower() == \"db\":\n",
    "            logging.info(f\"===== Start Extracting {data_name} data =====\")\n",
    "\n",
    "            df = spark.read.jdbc(\n",
    "                url=jdbc_url, table=data_name, properties=connection_properties\n",
    "            )\n",
    "\n",
    "            logging.info(f\"===== Finish Extracting {data_name} data =====\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Format data not supported yet\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"====== Failed to Extract Data ======\")\n",
    "        logging.error(e)\n",
    "\n",
    "        raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0b21b-d811-4e57-8b09-ba6e8e11152f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7ff55bc2-469f-43a7-954c-6e21adc6f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, round, regexp_replace\n",
    "\n",
    "def casting_data_types(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to cast data types based on table name.\n",
    "    \"\"\"\n",
    "    casting_mappings = {\n",
    "        \"marketing_campaign_deposit\": {\n",
    "            \"balance\": (\"int\", \"\\\\$\"),\n",
    "            \"duration_in_year\": (\"int\", None, \"duration\", 365)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if table_name in casting_mappings:\n",
    "        for col_name, cast_info in casting_mappings[table_name].items():\n",
    "            if len(cast_info) == 2 and cast_info[1]:\n",
    "                df = df.withColumn(col_name, regexp_replace(col(col_name), cast_info[1], \"\").cast(cast_info[0]))\n",
    "            elif len(cast_info) == 4:\n",
    "                df = df.withColumn(col_name, round(col(cast_info[2]) / cast_info[3]).cast(cast_info[0]))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513fbb2f-1dda-48de-912e-db68c60b4040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9bc85f67-dcb1-4531-8900-75208eca9778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "def clean_data(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to clean specific characters from the data.\n",
    "    \"\"\"\n",
    "    cleaning_mappings = {\n",
    "        \"marketing_campaign_deposit\": {\"balance\": \"\\$\"}\n",
    "    }\n",
    "    \n",
    "    if table_name in cleaning_mappings:\n",
    "        for col_name, pattern in cleaning_mappings[table_name].items():\n",
    "            df = df.withColumn(col_name, regexp_replace(col(col_name), pattern, \"\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e0f32b-9bb0-42cf-9eed-17cfbf9671ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b9ce53f8-8fb2-43d4-bc2c-ede4fa9d9125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_date, col, when, expr, length, lit, concat, substring\n",
    "\n",
    "def convert_date_columns(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to convert date and time columns based on table name.\n",
    "    \"\"\"\n",
    "    if table_name == \"transactions\":\n",
    "        # Convert TransactionDate from d/M/yy to YYYY/MM/DD\n",
    "        df = df.withColumn(\"TransactionDate\", to_date(col(\"TransactionDate\"), \"d/M/yy\"))\n",
    "        \n",
    "        # Convert TransactionTime from HHMMSS to HH:MM:SS\n",
    "        df = df.withColumn(\"TransactionTime\", expr(\n",
    "            \"substring(TransactionTime, 1, 2) || ':' || \"\n",
    "            \"substring(TransactionTime, 3, 2) || ':' || \"\n",
    "            \"substring(TransactionTime, 5, 2)\"\n",
    "        ))\n",
    "    \n",
    "    if table_name == \"customers\":\n",
    "        # Convert CustomerDOB with year correction\n",
    "        df = df.withColumn(\n",
    "            \"birth_date_temp\",\n",
    "            when(substring(col(\"CustomerDOB\"), 7, 2).cast(\"int\") > 25,\n",
    "                 concat(substring(col(\"CustomerDOB\"), 1, 6), lit(\"19\"), substring(col(\"CustomerDOB\"), 7, 2))\n",
    "                 ).otherwise(\n",
    "                concat(substring(col(\"CustomerDOB\"), 1, 6), lit(\"20\"), substring(col(\"CustomerDOB\"), 7, 2))\n",
    "            )\n",
    "        )\n",
    "        df = df.withColumn(\"birth_date\", to_date(col(\"birth_date_temp\"), \"d/M/yyyy\")).drop(\"birth_date_temp\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a989990-ecf4-4d35-bb15-c172246546b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9f694e3d-41dc-415d-ab05-66291c18a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_date, col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def convert_date_columns(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to convert date columns based on table name.\n",
    "    \"\"\"\n",
    "    date_mappings = {\n",
    "        \"customers\": {\"CustomerDOB\": \"birth_date\"},\n",
    "        \"transactions\": {\"TransactionDate\": \"transaction_date\"}\n",
    "    }\n",
    "    \n",
    "    if table_name in date_mappings:\n",
    "        for old_col, new_col in date_mappings[table_name].items():\n",
    "            if old_col == \"CustomerDOB\":\n",
    "                # Convert CustomerDOB to correct format using UDF\n",
    "                df = df.withColumn(new_col, to_date(convert_year_format_udf(col(old_col)), \"d/M/yyyy\"))\n",
    "            else:\n",
    "                # Convert TransactionDate to correct format\n",
    "                df = df.withColumn(new_col, to_date(convert_year_format_udf(col(old_col)), \"d/M/yyyy\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_year_format(date_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Function to convert CustomerDOB to correct format.\n",
    "    \"\"\"\n",
    "    if date_str:\n",
    "        parts = date_str.split('/')\n",
    "        if len(parts) == 3:\n",
    "            day, month, year = parts\n",
    "            if int(year) > 25:\n",
    "                year = \"19\" + year\n",
    "            else:\n",
    "                year = \"20\" + year\n",
    "            return f\"{day}/{month}/{year}\"\n",
    "    return date_str\n",
    "\n",
    "# Register UDF\n",
    "convert_year_format_udf = udf(convert_year_format, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d1f9d-bfe3-4ccb-a07d-314c45e8ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1b9c8954-b3e1-4718-80a1-d049d800ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, to_date, concat_ws, substring, when, lit\n",
    "\n",
    "def convert_date_columns(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to convert date columns based on table name.\n",
    "    \"\"\"\n",
    "    date_mappings = {\n",
    "        \"customers\": {\"CustomerDOB\": \"birth_date\"},\n",
    "        \"transactions\": {\"TransactionDate\": \"transaction_date\"}\n",
    "    }\n",
    "    \n",
    "    if table_name in date_mappings:\n",
    "        for old_col, new_col in date_mappings[table_name].items():\n",
    "            if old_col == \"CustomerDOB\":\n",
    "                # Convert CustomerDOB to correct format (d/M/yyyy)\n",
    "                df = df.withColumn(\n",
    "                    new_col,\n",
    "                    to_date(\n",
    "                        concat_ws(\n",
    "                            '/',\n",
    "                            substring(col(old_col), 1, 2),  # Day\n",
    "                            substring(col(old_col), 4, 2),  # Month\n",
    "                            concat_ws(\n",
    "                                '',\n",
    "                                when(substring(col(old_col), 7, 2).cast('int') > 25, lit('19'))\n",
    "                                .otherwise(lit('20')),\n",
    "                                substring(col(old_col), 7, 2)  # Year\n",
    "                            )\n",
    "                        ),\n",
    "                        \"d/M/yyyy\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # Convert TransactionDate to correct format (d/M/yyyy)\n",
    "                df = df.withColumn(\n",
    "                    new_col,\n",
    "                    to_date(\n",
    "                        concat_ws(\n",
    "                            '/',\n",
    "                            substring(col(old_col), 1, 2),  # Day\n",
    "                            substring(col(old_col), 4, 2),  # Month\n",
    "                            concat_ws(\n",
    "                                '',\n",
    "                                when(substring(col(old_col), 7, 2).cast('int') > 25, lit('19'))\n",
    "                                .otherwise(lit('20')),\n",
    "                                substring(col(old_col), 7, 2)  # Year\n",
    "                            )\n",
    "                        ),\n",
    "                        \"d/M/yyyy\"\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_transaction_time(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to convert TransactionTime to HH:MM:SS format.\n",
    "    Handles cases where the time string has 5 digits (e.g., 014103 -> 01:41:03).\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\n",
    "        \"TransactionTime\",\n",
    "        concat_ws(\n",
    "            ':',\n",
    "            when(length(col(\"TransactionTime\")) == 5, concat_ws('', lit('0'), substring(col(\"TransactionTime\"), 1, 1)))  # Add leading zero for 5-digit times\n",
    "            .otherwise(substring(col(\"TransactionTime\"), 1, 2)),  # HH\n",
    "            when(length(col(\"TransactionTime\")) == 5, substring(col(\"TransactionTime\"), 2, 2))  # MM for 5-digit times\n",
    "            .otherwise(substring(col(\"TransactionTime\"), 3, 2)),  # MM\n",
    "            when(length(col(\"TransactionTime\")) == 5, substring(col(\"TransactionTime\"), 4, 2))  # SS for 5-digit times\n",
    "            .otherwise(substring(col(\"TransactionTime\"), 5, 2))   # SS\n",
    "        )\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409361ef-d132-420e-839d-3cf2490177f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c241d5a-db09-4765-8840-9716aed9f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e049e-a45b-4f00-be35-914900d8cb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "88aa100b-f1f6-46d4-ad84-001fc0db96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_date, col, when, concat, substring, lit, regexp_extract\n",
    "\n",
    "def convert_date_columns(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to convert date and time columns based on table name.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        table_name: Name of the table being processed\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with converted date and time columns\n",
    "    \"\"\"\n",
    "    if table_name == \"transactions\":\n",
    "        # Convert TransactionDate from d/M/yy to YYYY/MM/DD\n",
    "        df = df.withColumn(\"transaction_date\", to_date(col(\"TransactionDate\"), \"d/M/yy\"))\n",
    "        \n",
    "        # Convert TransactionTime from HHMMSS to HH:MM:SS format using lpad\n",
    "        df = df.withColumn(\"padded_time\", lpad(col(\"TransactionTime\"), 6, \"0\"))\n",
    "        \n",
    "        # Then convert to HH:MM:SS format\n",
    "        df = df.withColumn(\n",
    "            \"transaction_time\",\n",
    "            concat(\n",
    "                substring(col(\"padded_time\"), 1, 2), lit(\":\"),\n",
    "                substring(col(\"padded_time\"), 3, 2), lit(\":\"),\n",
    "                substring(col(\"padded_time\"), 5, 2)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Drop the temporary column\n",
    "        df = df.drop(\"padded_time\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    elif table_name == \"customers\":\n",
    "        # Convert CustomerDOB with year > 25 check using regex for safer extraction\n",
    "        df = df.withColumn(\n",
    "            \"birth_date\",\n",
    "            when(\n",
    "                regexp_extract(col(\"CustomerDOB\"), \"(\\\\d{1,2})/(\\\\d{1,2})/(\\\\d{2})\", 3).cast(\"int\") > 25,\n",
    "                to_date(\n",
    "                    concat(\n",
    "                        regexp_extract(col(\"CustomerDOB\"), \"(\\\\d{1,2})/(\\\\d{1,2})/\", 1), \n",
    "                        lit(\"/\"),\n",
    "                        regexp_extract(col(\"CustomerDOB\"), \"\\\\d{1,2}/(\\\\d{1,2})/\", 1),\n",
    "                        lit(\"/19\"),\n",
    "                        regexp_extract(col(\"CustomerDOB\"), \"\\\\d{1,2}/\\\\d{1,2}/(\\\\d{2})\", 1)\n",
    "                    ),\n",
    "                    \"d/M/yyyy\"\n",
    "                )\n",
    "            ).otherwise(to_date(col(\"CustomerDOB\"), \"d/M/yy\"))\n",
    "        )\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "f8092d4c-be8b-4787-b417-af35b4fc4c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_date, col, when, concat, substring, lit, regexp_extract\n",
    "\n",
    "def convert_date_columns(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to convert date and time columns based on table name.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        table_name: Name of the table being processed\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with converted date and time columns\n",
    "    \"\"\"\n",
    "    if table_name == \"transactions\":\n",
    "        # Convert TransactionDate from d/M/yy to YYYY/MM/DD\n",
    "        df = df.withColumn(\"transaction_date\", to_date(col(\"TransactionDate\"), \"d/M/yy\"))\n",
    "        \n",
    "        # Convert TransactionTime from HHMMSS to HH:MM:SS format using lpad\n",
    "        df = df.withColumn(\"padded_time\", lpad(col(\"TransactionTime\"), 6, \"0\"))\n",
    "        \n",
    "        # Then convert to HH:MM:SS format\n",
    "        df = df.withColumn(\n",
    "            \"transaction_time\",\n",
    "            when(length(col(time_col)) == 6,\n",
    "                 concat(\n",
    "                     regexp_extract(col(time_col), \"^(\\\\d{2})\", 1), lit(\":\"),\n",
    "                     regexp_extract(col(time_col), \"^\\\\d{2}(\\\\d{2})\", 1), lit(\":\"),\n",
    "                     regexp_extract(col(time_col), \"^\\\\d{4}(\\\\d{2})\", 1)\n",
    "                 ))\n",
    "            .when(length(col(time_col)) == 5,\n",
    "                  concat(\n",
    "                      lit(\"0\"), regexp_extract(col(time_col), \"^(\\\\d{1})\", 1), lit(\":\"),\n",
    "                      regexp_extract(col(time_col), \"^\\\\d{1}(\\\\d{2})\", 1), lit(\":\"),\n",
    "                      regexp_extract(col(time_col), \"^\\\\d{3}(\\\\d{2})\", 1)\n",
    "                  ))\n",
    "            .when(length(col(time_col)) == 4,\n",
    "                  concat(\n",
    "                      lit(\"00\"), lit(\":\"), regexp_extract(col(time_col), \"^(\\\\d{2})\", 1), lit(\":\"),\n",
    "                      regexp_extract(col(time_col), \"^\\\\d{2}(\\\\d{2})\", 1)\n",
    "                  ))\n",
    "            .when(length(col(time_col)) == 3,\n",
    "                  concat(\n",
    "                      lit(\"00\"), lit(\":\"), lit(\"0\"), regexp_extract(col(time_col), \"^(\\\\d{1})\", 1), lit(\":\"),\n",
    "                      regexp_extract(col(time_col), \"^\\\\d{1}(\\\\d{2})\", 1)\n",
    "                  ))\n",
    "            .when(length(col(time_col)) == 2,\n",
    "                  concat(\n",
    "                      lit(\"00\"), lit(\":\"), lit(\"00\"), lit(\":\"), regexp_extract(col(time_col), \"^(\\\\d{2})\", 1)\n",
    "                  ))\n",
    "            .when(length(col(time_col)) == 1,\n",
    "                  concat(\n",
    "                      lit(\"00\"), lit(\":\"), lit(\"00\"), lit(\":\"), lit(\"0\"), regexp_extract(col(time_col), \"^(\\\\d{1})\", 1)\n",
    "                  ))\n",
    "            .otherwise(lit(\"00:00:00\"))\n",
    "        )\n",
    "        \n",
    "    elif table_name == \"customers\":\n",
    "        # Convert CustomerDOB with year > 25 check using regex for safer extraction\n",
    "        df = df.withColumn(\n",
    "            \"birth_date\",\n",
    "            when(\n",
    "                regexp_extract(col(\"CustomerDOB\"), \"(\\\\d{1,2})/(\\\\d{1,2})/(\\\\d{2})\", 3).cast(\"int\") > 25,\n",
    "                to_date(\n",
    "                    concat(\n",
    "                        regexp_extract(col(\"CustomerDOB\"), \"(\\\\d{1,2})/(\\\\d{1,2})/\", 1), \n",
    "                        lit(\"/\"),\n",
    "                        regexp_extract(col(\"CustomerDOB\"), \"\\\\d{1,2}/(\\\\d{1,2})/\", 1),\n",
    "                        lit(\"/19\"),\n",
    "                        regexp_extract(col(\"CustomerDOB\"), \"\\\\d{1,2}/\\\\d{1,2}/(\\\\d{2})\", 1)\n",
    "                    ),\n",
    "                    \"d/M/yyyy\"\n",
    "                )\n",
    "            ).otherwise(to_date(col(\"CustomerDOB\"), \"d/M/yy\"))\n",
    "        )\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "2e905568-1330-4800-9544-aba55519f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_date, col, when, concat, substring, lit, regexp_extract\n",
    "\n",
    "def convert_date_columns(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to convert date and time columns based on table name.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        table_name: Name of the table being processed\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with converted date and time columns\n",
    "    \"\"\"\n",
    "    if table_name == \"transactions\":\n",
    "        # Convert TransactionDate from d/M/yy to YYYY/MM/DD\n",
    "        df = df.withColumn(\"transaction_date\", to_date(col(\"TransactionDate\"), \"d/M/yy\"))\n",
    "        \n",
    "        # Convert TransactionTime from HHMMSS to HH:MM:SS format using safe pattern matching\n",
    "        df = df.withColumn(\n",
    "            \"transaction_time\",\n",
    "            when(length(col(\"TransactionTime\")) == 6,\n",
    "                 # 123456 → 12:34:56\n",
    "                 concat(\n",
    "                     substring(col(\"TransactionTime\"), 1, 2), lit(\":\"),\n",
    "                     substring(col(\"TransactionTime\"), 3, 2), lit(\":\"),\n",
    "                     substring(col(\"TransactionTime\"), 5, 2)\n",
    "                 )\n",
    "            ).when(length(col(\"TransactionTime\")) == 5,\n",
    "                 # 12345 → 01:23:45\n",
    "                 concat(\n",
    "                     lit(\"0\"), substring(col(\"TransactionTime\"), 1, 1), lit(\":\"),\n",
    "                     substring(col(\"TransactionTime\"), 2, 2), lit(\":\"),\n",
    "                     substring(col(\"TransactionTime\"), 4, 2)\n",
    "                 )\n",
    "            ).when(length(col(\"TransactionTime\")) == 4,\n",
    "                 # 1234 → 00:12:34\n",
    "                 concat(\n",
    "                     lit(\"00\"), lit(\":\"),\n",
    "                     substring(col(\"TransactionTime\"), 1, 2), lit(\":\"),\n",
    "                     substring(col(\"TransactionTime\"), 3, 2)\n",
    "                 )\n",
    "            ).when(length(col(\"TransactionTime\")) == 3,\n",
    "                 # 123 → 00:01:23\n",
    "                 concat(\n",
    "                     lit(\"00\"), lit(\":\"),\n",
    "                     lit(\"0\"), substring(col(\"TransactionTime\"), 1, 1), lit(\":\"),\n",
    "                     substring(col(\"TransactionTime\"), 2, 2)\n",
    "                 )\n",
    "            ).when(length(col(\"TransactionTime\")) == 2,\n",
    "                 # 12 → 00:00:12\n",
    "                 concat(\n",
    "                     lit(\"00\"), lit(\":\"),\n",
    "                     lit(\"00\"), lit(\":\"),\n",
    "                     substring(col(\"TransactionTime\"), 1, 2)\n",
    "                 )\n",
    "            ).when(length(col(\"TransactionTime\")) == 1,\n",
    "                 # 1 → 00:00:01\n",
    "                 concat(\n",
    "                     lit(\"00\"), lit(\":\"),\n",
    "                     lit(\"00\"), lit(\":\"),\n",
    "                     lit(\"0\"), substring(col(\"TransactionTime\"), 1, 1)\n",
    "                 )\n",
    "            ).otherwise(lit(\"00:00:00\"))  # Handle empty or null values\n",
    "        )\n",
    "\n",
    "    elif table_name == \"customers\":\n",
    "        # Convert CustomerDOB with year > 25 check using regex for safer extraction\n",
    "        df = df.withColumn(\n",
    "            \"birth_date\",\n",
    "            when(\n",
    "                regexp_extract(col(\"CustomerDOB\"), \"(\\\\d{1,2})/(\\\\d{1,2})/(\\\\d{2})\", 3).cast(\"int\") > 25,\n",
    "                to_date(\n",
    "                    concat(\n",
    "                        regexp_extract(col(\"CustomerDOB\"), \"(\\\\d{1,2})/(\\\\d{1,2})/\", 1), \n",
    "                        lit(\"/\"),\n",
    "                        regexp_extract(col(\"CustomerDOB\"), \"\\\\d{1,2}/(\\\\d{1,2})/\", 1),\n",
    "                        lit(\"/19\"),\n",
    "                        regexp_extract(col(\"CustomerDOB\"), \"\\\\d{1,2}/\\\\d{1,2}/(\\\\d{2})\", 1)\n",
    "                    ),\n",
    "                    \"d/M/yyyy\"\n",
    "                )\n",
    "            ).otherwise(to_date(col(\"CustomerDOB\"), \"d/M/yy\"))\n",
    "        )\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575c157-ec65-46ea-978f-e90037e1bba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "edd90c00-e2a4-4f2f-885f-f8f8e714bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "def feature_engineering(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to apply feature engineering based on table name.\n",
    "    \"\"\"\n",
    "    if table_name == \"customers\":\n",
    "        df = df.withColumn(\"gender\", when(col(\"CustGender\") == \"M\", \"Male\")\n",
    "                                      .when(col(\"CustGender\") == \"F\", \"Female\")\n",
    "                                      .otherwise(\"Other\"))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018dacf-a313-4b37-81a3-cec5cedddbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "2cef0b24-c6df-4e7c-a0e7-43e2501ca74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def rename_columns(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to rename columns based on the table name.\n",
    "    \"\"\"\n",
    "    rename_mappings = {\n",
    "        \"marketing_campaign_deposit\": {\n",
    "            \"pdays\": \"days_since_last_campaign\",\n",
    "            \"previous\": \"previous_campaign_contacts\",\n",
    "            \"poutcome\": \"previous_campaign_outcome\"\n",
    "        },\n",
    "        \"customers\": {\n",
    "            \"CustomerID\": \"customer_id\",\n",
    "            \"CustLocation\": \"location\",\n",
    "            \"CustAccountBalance\": \"account_balance\"\n",
    "        },\n",
    "        \"transactions\": {\n",
    "            \"TransactionID\": \"transaction_id\",\n",
    "            # \"TransactionTime\": \"transaction_time\",\n",
    "            \"TransactionAmount (INR)\": \"transaction_amount\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if table_name in rename_mappings:\n",
    "        for old_col, new_col in rename_mappings[table_name].items():\n",
    "            df = df.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48570b43-77da-49f1-954e-9c364559c718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "01202ccd-8fe2-4a5d-9124-590f806c1692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "logging_process()\n",
    "\n",
    "def transform_data(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to apply all transformation steps on the dataframe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"===== Start Transforming Data for {table_name} =====\")\n",
    "        df = rename_columns(df, table_name)\n",
    "        df = convert_date_columns(df, table_name)\n",
    "        # df = convert_transaction_time(df)\n",
    "        df = casting_data_types(df, table_name)\n",
    "        df = feature_engineering(df, table_name)\n",
    "        df = clean_data(df, table_name)\n",
    "        logging.info(f\"===== Finished Transforming Data for {table_name} =====\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"===== Failed to Transform Data for {table_name} =====\")\n",
    "        logging.error(e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a23ff2-690f-4e4f-9342-e9d0f4f1948d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "941e0675-eef7-4967-8004-18ef0ec982f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pyspark\n",
    "\n",
    "logging_process()\n",
    "\n",
    "\n",
    "def load_data(df_result: pyspark.sql.DataFrame, table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Function that used to dump the result to the database using PySpark\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_result (pyspark.sql.DataFrame): final result of pyspark movie dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # set variable for database\n",
    "        DB_URL = \"jdbc:postgresql://data_warehouse:5432/data_warehouse\"\n",
    "        DB_USER = \"postgres\"\n",
    "        DB_PASS = \"postgres\"\n",
    "\n",
    "        # set config\n",
    "        jdbc_url = DB_URL\n",
    "        connection_properties = {\n",
    "            \"user\": DB_USER,\n",
    "            \"password\": DB_PASS,\n",
    "            \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "        }\n",
    "\n",
    "        logging.info(\"===== Start Load data to the database =====\")\n",
    "\n",
    "        # load data\n",
    "        df_result.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table_name,\n",
    "            mode=\"overwrite\",    # sql untuk method truncate terlebih dahulu \n",
    "            properties=connection_properties,\n",
    "        )\n",
    "\n",
    "        logging.info(\"===== Finish Load data to the database =====\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"===== Failed Load data to the database =====\")\n",
    "        logging.error(e)\n",
    "        raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979a1ff-f8cf-41e4-a5cb-35a3b335591d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3bde89-97df-4fa4-b0ea-7327a40121d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "# Initialize logging\n",
    "logging_process()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"===== Start Banking Data Pipeline =====\")\n",
    "\n",
    "    try:\n",
    "        # Extract data from CSV and database\n",
    "        df_transactions = extract_data(data_name=\"new_bank_transaction\", format_data=\"csv\")\n",
    "        df_customers = extract_data(data_name=\"new_bank_transaction\", format_data=\"csv\")\n",
    "        df_marketing = extract_data(data_name=\"marketing_campaign_deposit\", format_data=\"db\")\n",
    "        df_education = extract_data(data_name=\"education_status\", format_data=\"db\")\n",
    "        df_marital = extract_data(data_name=\"marital_status\", format_data=\"db\")\n",
    "\n",
    "        # Transform each dataset separately\n",
    "        df_transactions = transform_data(df_transactions, \"transactions\")\n",
    "        df_customers = transform_data(df_customers, \"customers\")\n",
    "        df_marketing = transform_data(df_marketing, \"marketing_campaign_deposit\")\n",
    "        df_education = transform_data(df_education, \"education_status\")\n",
    "        df_marital = transform_data(df_marital, \"marital_status\")\n",
    "\n",
    "        # Load each transformed dataset into the data warehouse\n",
    "        load_data(df_transactions, table_name=\"transactions\")\n",
    "        load_data(df_customers, table_name=\"customers\")\n",
    "        load_data(df_marketing, table_name=\"marketing_campaign_deposit\")\n",
    "        load_data(df_education, table_name=\"education_status\")\n",
    "        load_data(df_marital, table_name=\"marital_status\")\n",
    "\n",
    "        logging.info(\"===== Finish Banking Data Pipeline =====\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"===== Data Pipeline Failed =====\")\n",
    "        logging.error(e)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "c1406ed2-f5a4-4f66-8c5b-4fcc858d364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = extract_data(data_name=\"new_bank_transaction\", format_data=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "62f14498-9eae-41c6-b182-09118266ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = transform_data(df_customers, \"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "de729f08-23a1-400c-a34e-e04e506c9a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[TransactionID: string, customer_id: string, CustomerDOB: string, CustGender: string, location: string, account_balance: string, TransactionDate: string, TransactionTime: string, TransactionAmount (INR): string, gender: string]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b914d5c6-5236-494e-a7ab-fbfdbc9f4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "7b652789-c12b-45c2-ac8c-91f34b631b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(df_customers, table_name=\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b359caeb-c53e-48b4-ae3d-536a1410a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers.show(83, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "83b4c7cb-2059-49a5-b6db-0f3f6e0dbffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " TransactionID           | T642232    \n",
      " customer_id             | C1010028   \n",
      " CustomerDOB             | 25/8/88    \n",
      " CustGender              | F          \n",
      " location                | DELHI      \n",
      " account_balance         | 296828.37  \n",
      " TransactionDate         | 29/8/16    \n",
      " TransactionTime         | 09:52:12   \n",
      " TransactionAmount (INR) | 557        \n",
      " birth_date              | 1988-08-25 \n",
      " gender                  | Female     \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.show(1, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa64c36-0025-44dc-96f1-b66bacdc5f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "66a2a4df-2dc2-478b-b39d-e43be222d3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " TransactionID           | T986198    \n",
      " customer_id             | C1010372_2 \n",
      " CustomerDOB             | 29/5/92    \n",
      " CustGender              | F          \n",
      " location                | GURGAON    \n",
      " account_balance         | 3762.92    \n",
      " TransactionDate         | 15/9/16    \n",
      " TransactionTime         | 5919       \n",
      " TransactionAmount (INR) | 94         \n",
      " birth_date              | 1992-05-29 \n",
      " gender                  | Female     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.filter(col(\"TransactionID\") == \"T986198\").show(truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "fa81fc6a-1cba-43bc-9cba-27afa8462918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " TransactionID           | T986198    \n",
      " customer_id             | C1010372_2 \n",
      " CustomerDOB             | 29/5/92    \n",
      " CustGender              | F          \n",
      " location                | GURGAON    \n",
      " account_balance         | 3762.92    \n",
      " TransactionDate         | 15/9/16    \n",
      " TransactionTime         | 59:19:     \n",
      " TransactionAmount (INR) | 94         \n",
      " birth_date              | 1992-05-29 \n",
      " gender                  | Female     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.filter(col(\"TransactionID\") == \"T986198\").show(truncate = False, vertical = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fdba5b95-a6ed-494e-8f19-b219ddaad9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " TransactionID           | T642232     \n",
      " customer_id             | C1010028    \n",
      " CustomerDOB             | 25/8/88     \n",
      " CustGender              | F           \n",
      " location                | DELHI       \n",
      " account_balance         | 296828.37   \n",
      " TransactionDate         | 29/8/16     \n",
      " TransactionTime         | 95212       \n",
      " TransactionAmount (INR) | 557         \n",
      " birth_date              | 8208-08-25  \n",
      " gender                  | Female      \n",
      "-RECORD 1------------------------------\n",
      " TransactionID           | T87414      \n",
      " customer_id             | C1010035    \n",
      " CustomerDOB             | 2/3/92      \n",
      " CustGender              | M           \n",
      " location                | MUMBAI      \n",
      " account_balance         | 7284.42     \n",
      " TransactionDate         | 1/8/16      \n",
      " TransactionTime         | 111917      \n",
      " TransactionAmount (INR) | 50          \n",
      " birth_date              | 9220-03-02  \n",
      " gender                  | Male        \n",
      "-RECORD 2------------------------------\n",
      " TransactionID           | T560676     \n",
      " customer_id             | C1010035_2  \n",
      " CustomerDOB             | 9/6/80      \n",
      " CustGender              | M           \n",
      " location                | NAVI MUMBAI \n",
      " account_balance         | 378013.09   \n",
      " TransactionDate         | 27/8/16     \n",
      " TransactionTime         | 185011      \n",
      " TransactionAmount (INR) | 700         \n",
      " birth_date              | 8020-06-09  \n",
      " gender                  | Male        \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers2.show(3, truncate = False, vertical = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
