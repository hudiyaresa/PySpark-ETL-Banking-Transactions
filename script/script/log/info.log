2025-03-18 22:17:36,165 - INFO - ===== Start Banking Data Pipeline =====
2025-03-18 22:17:36,272 - INFO - ===== Start Extracting transactions data =====
2025-03-18 22:17:36,366 - ERROR - ====== Failed to Extract Data ======
2025-03-18 22:17:36,366 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/transactions.csv.
2025-03-18 22:17:36,375 - ERROR - ===== Data Pipeline Failed =====
2025-03-18 22:17:36,375 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/transactions.csv.
2025-03-18 22:17:57,541 - INFO - ===== Start Banking Data Pipeline =====
2025-03-18 22:17:57,551 - INFO - ===== Start Extracting customers data =====
2025-03-18 22:17:57,732 - ERROR - ====== Failed to Extract Data ======
2025-03-18 22:17:57,733 - ERROR - An error occurred while calling o471.jdbc.
: org.postgresql.util.PSQLException: ERROR: relation "customers" does not exist
  Position: 15
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:368)
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:498)
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:415)
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:190)
	at org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:134)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:68)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)

2025-03-18 22:17:57,734 - ERROR - ===== Data Pipeline Failed =====
2025-03-18 22:17:57,735 - ERROR - An error occurred while calling o471.jdbc.
: org.postgresql.util.PSQLException: ERROR: relation "customers" does not exist
  Position: 15
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:368)
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:498)
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:415)
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:190)
	at org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:134)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:68)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)

2025-03-18 22:25:02,122 - INFO - ===== Start Banking Data Pipeline =====
2025-03-18 22:25:02,128 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-18 22:25:02,266 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-18 22:25:02,272 - INFO - ===== Start Extracting education_status data =====
2025-03-18 22:25:02,292 - INFO - ===== Finish Extracting education_status data =====
2025-03-18 22:25:02,294 - INFO - ===== Start Extracting marital_status data =====
2025-03-18 22:25:02,313 - INFO - ===== Finish Extracting marital_status data =====
2025-03-18 22:25:02,314 - ERROR - ===== Data Pipeline Failed =====
2025-03-18 22:25:02,315 - ERROR - transform_data() got an unexpected keyword argument 'df_marketing'
2025-03-18 22:26:27,406 - INFO - ===== Start Banking Data Pipeline =====
2025-03-18 22:26:27,412 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-18 22:26:27,456 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-18 22:26:27,458 - INFO - ===== Start Extracting education_status data =====
2025-03-18 22:26:27,482 - INFO - ===== Finish Extracting education_status data =====
2025-03-18 22:26:27,485 - INFO - ===== Start Extracting marital_status data =====
2025-03-18 22:26:27,507 - INFO - ===== Finish Extracting marital_status data =====
2025-03-18 22:26:27,508 - ERROR - ===== Data Pipeline Failed =====
2025-03-18 22:26:27,508 - ERROR - transform_data() got an unexpected keyword argument 'df_marketing'
2025-03-18 22:26:42,167 - INFO - ===== Start Banking Data Pipeline =====
2025-03-18 22:26:42,170 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-18 22:26:42,197 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-18 22:26:42,199 - INFO - ===== Start Extracting education_status data =====
2025-03-18 22:26:42,214 - INFO - ===== Finish Extracting education_status data =====
2025-03-18 22:26:42,217 - INFO - ===== Start Extracting marital_status data =====
2025-03-18 22:26:42,234 - INFO - ===== Finish Extracting marital_status data =====
2025-03-18 22:26:42,235 - ERROR - ===== Data Pipeline Failed =====
2025-03-18 22:26:42,235 - ERROR - transform_data() got an unexpected keyword argument 'df_marketing_campaign_deposit'
2025-03-18 22:44:54,096 - INFO - ===== Start Banking Data Pipeline =====
2025-03-18 22:44:54,218 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-18 22:44:54,526 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-18 22:44:54,529 - INFO - ===== Start Extracting education_status data =====
2025-03-18 22:44:54,557 - INFO - ===== Finish Extracting education_status data =====
2025-03-18 22:44:54,562 - INFO - ===== Start Extracting marital_status data =====
2025-03-18 22:44:54,593 - INFO - ===== Finish Extracting marital_status data =====
2025-03-18 22:44:54,594 - INFO - ===== Start Transforming Data for marketing_campaign_deposit =====
2025-03-18 22:44:55,076 - INFO - ===== Finished Transforming Data for marketing_campaign_deposit =====
2025-03-18 22:44:55,077 - INFO - ===== Start Transforming Data for education_status =====
2025-03-18 22:44:55,078 - INFO - ===== Finished Transforming Data for education_status =====
2025-03-18 22:44:55,078 - INFO - ===== Start Transforming Data for marital_status =====
2025-03-18 22:44:55,079 - INFO - ===== Finished Transforming Data for marital_status =====
2025-03-18 22:44:55,080 - ERROR - ===== Data Pipeline Failed =====
2025-03-18 22:44:55,081 - ERROR - name 'load_data' is not defined
2025-03-18 22:45:31,706 - INFO - ===== Start Banking Data Pipeline =====
2025-03-18 22:45:31,712 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-18 22:45:31,746 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-18 22:45:31,751 - INFO - ===== Start Extracting education_status data =====
2025-03-18 22:45:31,773 - INFO - ===== Finish Extracting education_status data =====
2025-03-18 22:45:31,777 - INFO - ===== Start Extracting marital_status data =====
2025-03-18 22:45:31,798 - INFO - ===== Finish Extracting marital_status data =====
2025-03-18 22:45:31,798 - INFO - ===== Start Transforming Data for marketing_campaign_deposit =====
2025-03-18 22:45:31,883 - INFO - ===== Finished Transforming Data for marketing_campaign_deposit =====
2025-03-18 22:45:31,884 - INFO - ===== Start Transforming Data for education_status =====
2025-03-18 22:45:31,885 - INFO - ===== Finished Transforming Data for education_status =====
2025-03-18 22:45:31,886 - INFO - ===== Start Transforming Data for marital_status =====
2025-03-18 22:45:31,887 - INFO - ===== Finished Transforming Data for marital_status =====
2025-03-18 22:45:31,888 - ERROR - ===== Data Pipeline Failed =====
2025-03-18 22:45:31,888 - ERROR - name 'df_result' is not defined
2025-03-18 22:47:11,928 - INFO - ===== Start Banking Data Pipeline =====
2025-03-18 22:47:11,937 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-18 22:47:11,984 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-18 22:47:11,988 - INFO - ===== Start Extracting education_status data =====
2025-03-18 22:47:12,015 - INFO - ===== Finish Extracting education_status data =====
2025-03-18 22:47:12,018 - INFO - ===== Start Extracting marital_status data =====
2025-03-18 22:47:12,046 - INFO - ===== Finish Extracting marital_status data =====
2025-03-18 22:47:12,046 - INFO - ===== Start Transforming Data for marketing_campaign_deposit =====
2025-03-18 22:47:12,146 - INFO - ===== Finished Transforming Data for marketing_campaign_deposit =====
2025-03-18 22:47:12,146 - INFO - ===== Start Transforming Data for education_status =====
2025-03-18 22:47:12,147 - INFO - ===== Finished Transforming Data for education_status =====
2025-03-18 22:47:12,147 - INFO - ===== Start Transforming Data for marital_status =====
2025-03-18 22:47:12,148 - INFO - ===== Finished Transforming Data for marital_status =====
2025-03-18 22:47:12,273 - INFO - ===== Start Load data to the database =====
2025-03-18 22:47:12,532 - ERROR - ===== Failed Load data to the database =====
2025-03-18 22:47:12,532 - ERROR - An error occurred while calling o629.jdbc.
: org.postgresql.util.PSQLException: ERROR: column "created_at" specified more than once
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:368)
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:498)
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:415)
	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:335)
	at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:321)
	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:297)
	at org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:270)
	at org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:191)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:921)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)

2025-03-18 22:47:12,534 - ERROR - ===== Data Pipeline Failed =====
2025-03-18 22:47:12,534 - ERROR - An error occurred while calling o629.jdbc.
: org.postgresql.util.PSQLException: ERROR: column "created_at" specified more than once
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:368)
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:498)
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:415)
	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:335)
	at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:321)
	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:297)
	at org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:270)
	at org.apache.spark.sql.jdbc.JdbcDialect.createTable(JdbcDialects.scala:191)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:921)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)

2025-03-18 22:48:30,420 - INFO - ===== Start Banking Data Pipeline =====
2025-03-18 22:48:30,426 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-18 22:48:30,478 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-18 22:48:30,484 - INFO - ===== Start Extracting education_status data =====
2025-03-18 22:48:30,524 - INFO - ===== Finish Extracting education_status data =====
2025-03-18 22:48:30,530 - INFO - ===== Start Extracting marital_status data =====
2025-03-18 22:48:30,568 - INFO - ===== Finish Extracting marital_status data =====
2025-03-18 22:48:30,568 - INFO - ===== Finish Banking Data Pipeline =====
2025-03-18 22:48:55,949 - INFO - ===== Start Banking Data Pipeline =====
2025-03-18 22:48:55,958 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-18 22:48:56,006 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-18 22:48:56,011 - INFO - ===== Start Extracting education_status data =====
2025-03-18 22:48:56,038 - INFO - ===== Finish Extracting education_status data =====
2025-03-18 22:48:56,042 - INFO - ===== Start Extracting marital_status data =====
2025-03-18 22:48:56,065 - INFO - ===== Finish Extracting marital_status data =====
2025-03-18 22:48:56,065 - INFO - ===== Start Transforming Data for marketing_campaign_deposit =====
2025-03-18 22:48:56,161 - INFO - ===== Finished Transforming Data for marketing_campaign_deposit =====
2025-03-18 22:48:56,162 - INFO - ===== Start Transforming Data for education_status =====
2025-03-18 22:48:56,163 - INFO - ===== Finished Transforming Data for education_status =====
2025-03-18 22:48:56,163 - INFO - ===== Start Transforming Data for marital_status =====
2025-03-18 22:48:56,164 - INFO - ===== Finished Transforming Data for marital_status =====
2025-03-18 22:48:56,164 - INFO - ===== Finish Banking Data Pipeline =====
2025-03-18 22:51:32,814 - INFO - ===== Start Banking Data Pipeline =====
2025-03-18 22:51:32,822 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-18 22:51:32,861 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-18 22:51:32,864 - INFO - ===== Start Extracting education_status data =====
2025-03-18 22:51:32,881 - INFO - ===== Finish Extracting education_status data =====
2025-03-18 22:51:32,884 - INFO - ===== Start Extracting marital_status data =====
2025-03-18 22:51:32,904 - INFO - ===== Finish Extracting marital_status data =====
2025-03-18 22:51:32,904 - INFO - ===== Start Transforming Data for marketing_campaign_deposit =====
2025-03-18 22:51:33,012 - INFO - ===== Finished Transforming Data for marketing_campaign_deposit =====
2025-03-18 22:51:33,013 - INFO - ===== Start Transforming Data for education_status =====
2025-03-18 22:51:33,014 - INFO - ===== Finished Transforming Data for education_status =====
2025-03-18 22:51:33,015 - INFO - ===== Start Transforming Data for marital_status =====
2025-03-18 22:51:33,015 - INFO - ===== Finished Transforming Data for marital_status =====
2025-03-18 22:51:33,016 - INFO - ===== Start Load data to the database =====
2025-03-18 22:51:35,718 - INFO - ===== Finish Load data to the database =====
2025-03-18 22:51:35,718 - INFO - ===== Start Load data to the database =====
2025-03-18 22:51:35,890 - INFO - ===== Finish Load data to the database =====
2025-03-18 22:51:35,891 - INFO - ===== Start Load data to the database =====
2025-03-18 22:51:36,041 - INFO - ===== Finish Load data to the database =====
2025-03-18 22:51:36,042 - INFO - ===== Finish Banking Data Pipeline =====
2025-03-18 22:52:49,127 - ERROR - No such comm target registered: jupyter.widget.control
2025-03-18 22:52:49,135 - WARNING - No such comm: 71232d3b-d90f-41ec-8e61-0f68fb0d220b
2025-03-19 13:16:33,670 - ERROR - No such comm target registered: jupyter.widget.control
2025-03-19 13:16:33,678 - WARNING - No such comm: 9dd05ff6-a474-4a87-a058-ef39e0b3b843
2025-03-19 14:01:37,585 - INFO - ===== Start Banking Data Pipeline =====
2025-03-19 14:01:37,617 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-19 14:01:37,695 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-19 14:01:37,699 - INFO - ===== Start Extracting education_status data =====
2025-03-19 14:01:37,724 - INFO - ===== Finish Extracting education_status data =====
2025-03-19 14:01:37,727 - INFO - ===== Start Extracting marital_status data =====
2025-03-19 14:01:37,761 - INFO - ===== Finish Extracting marital_status data =====
2025-03-19 14:01:37,762 - INFO - ===== Start Transforming Data for marketing_campaign_deposit =====
2025-03-19 14:01:37,827 - INFO - ===== Finished Transforming Data for marketing_campaign_deposit =====
2025-03-19 14:01:37,828 - INFO - ===== Start Transforming Data for education_status =====
2025-03-19 14:01:37,829 - INFO - ===== Finished Transforming Data for education_status =====
2025-03-19 14:01:37,829 - INFO - ===== Start Transforming Data for marital_status =====
2025-03-19 14:01:37,830 - INFO - ===== Finished Transforming Data for marital_status =====
2025-03-19 14:01:37,831 - INFO - ===== Start Load data to the database =====
2025-03-19 14:01:39,593 - INFO - ===== Finish Load data to the database =====
2025-03-19 14:01:39,593 - INFO - ===== Start Load data to the database =====
2025-03-19 14:01:39,695 - INFO - ===== Finish Load data to the database =====
2025-03-19 14:01:39,696 - INFO - ===== Start Load data to the database =====
2025-03-19 14:01:39,806 - INFO - ===== Finish Load data to the database =====
2025-03-19 14:01:39,806 - INFO - ===== Finish Banking Data Pipeline =====
2025-03-19 14:16:07,290 - INFO - ===== Start Banking Data Pipeline =====
2025-03-19 14:16:13,038 - INFO - ===== Start Extracting transactions data =====
2025-03-19 14:16:14,748 - ERROR - ====== Failed to Extract Data ======
2025-03-19 14:16:14,749 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/transactions.csv.
2025-03-19 14:16:14,776 - ERROR - ===== Data Pipeline Failed =====
2025-03-19 14:16:14,777 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/transactions.csv.
2025-03-19 14:16:14,833 - INFO - Closing down clientserver connection
2025-03-19 14:32:08,642 - INFO - ===== Start Banking Data Pipeline =====
2025-03-19 14:32:08,654 - INFO - ===== Start Extracting transactions data =====
2025-03-19 14:32:08,682 - ERROR - ====== Failed to Extract Data ======
2025-03-19 14:32:08,683 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/transactions.csv.
2025-03-19 14:32:08,690 - ERROR - ===== Data Pipeline Failed =====
2025-03-19 14:32:08,690 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/transactions.csv.
2025-03-19 14:35:49,278 - INFO - ===== Start Banking Data Pipeline =====
2025-03-19 14:35:49,286 - INFO - ===== Start Extracting transactions data =====
2025-03-19 14:35:49,316 - ERROR - ====== Failed to Extract Data ======
2025-03-19 14:35:49,317 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/transactions.csv.
2025-03-19 14:35:49,322 - ERROR - ===== Data Pipeline Failed =====
2025-03-19 14:35:49,323 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/transactions.csv.
2025-03-19 14:36:10,471 - INFO - ===== Start Banking Data Pipeline =====
2025-03-19 14:36:10,478 - INFO - ===== Start Extracting new_bank_transactions data =====
2025-03-19 14:36:10,506 - ERROR - ====== Failed to Extract Data ======
2025-03-19 14:36:10,507 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/new_bank_transactions.csv.
2025-03-19 14:36:10,514 - ERROR - ===== Data Pipeline Failed =====
2025-03-19 14:36:10,515 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/new_bank_transactions.csv.
2025-03-19 14:37:00,395 - INFO - ===== Start Banking Data Pipeline =====
2025-03-19 14:37:00,401 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 14:37:01,611 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 14:37:01,615 - INFO - ===== Start Extracting customers data =====
2025-03-19 14:37:01,636 - ERROR - ====== Failed to Extract Data ======
2025-03-19 14:37:01,637 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/customers.csv.
2025-03-19 14:37:01,640 - ERROR - ===== Data Pipeline Failed =====
2025-03-19 14:37:01,641 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/customers.csv.
2025-03-19 14:37:16,946 - INFO - ===== Start Banking Data Pipeline =====
2025-03-19 14:37:16,950 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 14:37:18,184 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 14:37:18,189 - INFO - ===== Start Extracting customers data =====
2025-03-19 14:37:18,213 - ERROR - ====== Failed to Extract Data ======
2025-03-19 14:37:18,214 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/customers.csv.
2025-03-19 14:37:18,218 - ERROR - ===== Data Pipeline Failed =====
2025-03-19 14:37:18,219 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/customers.csv.
2025-03-19 14:37:25,568 - INFO - ===== Start Banking Data Pipeline =====
2025-03-19 14:37:25,572 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 14:37:26,787 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 14:37:26,791 - INFO - ===== Start Extracting customers data =====
2025-03-19 14:37:26,813 - ERROR - ====== Failed to Extract Data ======
2025-03-19 14:37:26,814 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/customers.csv.
2025-03-19 14:37:26,818 - ERROR - ===== Data Pipeline Failed =====
2025-03-19 14:37:26,819 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/customers.csv.
2025-03-19 14:37:49,847 - INFO - ===== Start Banking Data Pipeline =====
2025-03-19 14:37:49,850 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 14:37:50,915 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 14:37:50,918 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-19 14:37:51,016 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-19 14:37:51,020 - INFO - ===== Start Extracting education_status data =====
2025-03-19 14:37:51,042 - INFO - ===== Finish Extracting education_status data =====
2025-03-19 14:37:51,046 - INFO - ===== Start Extracting marital_status data =====
2025-03-19 14:37:51,066 - INFO - ===== Finish Extracting marital_status data =====
2025-03-19 14:37:51,066 - INFO - ===== Start Transforming Data for transactions =====
2025-03-19 14:37:51,104 - INFO - ===== Finished Transforming Data for transactions =====
2025-03-19 14:37:51,104 - INFO - ===== Start Transforming Data for marketing_campaign_deposit =====
2025-03-19 14:37:51,171 - INFO - ===== Finished Transforming Data for marketing_campaign_deposit =====
2025-03-19 14:37:51,172 - INFO - ===== Start Transforming Data for education_status =====
2025-03-19 14:37:51,172 - INFO - ===== Finished Transforming Data for education_status =====
2025-03-19 14:37:51,173 - INFO - ===== Start Transforming Data for marital_status =====
2025-03-19 14:37:51,174 - INFO - ===== Finished Transforming Data for marital_status =====
2025-03-19 14:37:51,174 - INFO - ===== Start Load data to the database =====
2025-03-19 14:37:58,004 - INFO - ===== Finish Load data to the database =====
2025-03-19 14:37:58,004 - ERROR - ===== Data Pipeline Failed =====
2025-03-19 14:37:58,005 - ERROR - name 'df_customers' is not defined
2025-03-19 14:41:36,025 - INFO - ===== Start Banking Data Pipeline =====
2025-03-19 14:41:36,037 - INFO - ===== Start Extracting new_bank_transactions data =====
2025-03-19 14:41:36,081 - ERROR - ====== Failed to Extract Data ======
2025-03-19 14:41:36,082 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/new_bank_transactions.csv.
2025-03-19 14:41:36,087 - ERROR - ===== Data Pipeline Failed =====
2025-03-19 14:41:36,088 - ERROR - [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/data/new_bank_transactions.csv.
2025-03-19 14:41:53,304 - INFO - ===== Start Banking Data Pipeline =====
2025-03-19 14:41:53,313 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 14:41:54,399 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 14:41:54,402 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 14:41:55,432 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 14:41:55,436 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-19 14:41:55,470 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-19 14:41:55,474 - INFO - ===== Start Extracting education_status data =====
2025-03-19 14:41:55,495 - INFO - ===== Finish Extracting education_status data =====
2025-03-19 14:41:55,498 - INFO - ===== Start Extracting marital_status data =====
2025-03-19 14:41:55,517 - INFO - ===== Finish Extracting marital_status data =====
2025-03-19 14:41:55,518 - INFO - ===== Start Transforming Data for transactions =====
2025-03-19 14:41:55,534 - INFO - ===== Finished Transforming Data for transactions =====
2025-03-19 14:41:55,535 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 14:41:55,596 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 14:41:55,597 - INFO - ===== Start Transforming Data for marketing_campaign_deposit =====
2025-03-19 14:41:55,668 - INFO - ===== Finished Transforming Data for marketing_campaign_deposit =====
2025-03-19 14:41:55,668 - INFO - ===== Start Transforming Data for education_status =====
2025-03-19 14:41:55,669 - INFO - ===== Finished Transforming Data for education_status =====
2025-03-19 14:41:55,670 - INFO - ===== Start Transforming Data for marital_status =====
2025-03-19 14:41:55,670 - INFO - ===== Finished Transforming Data for marital_status =====
2025-03-19 14:41:55,671 - INFO - ===== Start Load data to the database =====
2025-03-19 14:42:02,090 - INFO - ===== Finish Load data to the database =====
2025-03-19 14:42:02,091 - INFO - ===== Start Load data to the database =====
2025-03-19 14:42:02,622 - ERROR - ===== Failed Load data to the database =====
2025-03-19 14:42:02,623 - ERROR - An error occurred while calling o1219.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 58.0 failed 1 times, most recent failure: Lost task 3.0 in stage 58.0 (TID 70) (pyspark executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '1/1/1800' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.time.format.DateTimeParseException: Text '1/1/1800' could not be parsed, unparsed text found at index 6
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2055)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:65)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '1/1/1800' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: java.time.format.DateTimeParseException: Text '1/1/1800' could not be parsed, unparsed text found at index 6
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2055)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 22 more

2025-03-19 14:42:02,669 - ERROR - ===== Data Pipeline Failed =====
2025-03-19 14:42:02,669 - ERROR - An error occurred while calling o1219.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 58.0 failed 1 times, most recent failure: Lost task 3.0 in stage 58.0 (TID 70) (pyspark executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '1/1/1800' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.time.format.DateTimeParseException: Text '1/1/1800' could not be parsed, unparsed text found at index 6
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2055)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:65)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '1/1/1800' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: java.time.format.DateTimeParseException: Text '1/1/1800' could not be parsed, unparsed text found at index 6
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2055)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 22 more

2025-03-19 14:52:02,860 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 14:52:04,028 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 14:52:09,656 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 14:52:09,702 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 15:47:41,154 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 15:47:43,172 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 15:47:43,885 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 15:47:44,266 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 15:48:03,275 - INFO - ===== Start Load data to the database =====
2025-03-19 15:48:04,283 - ERROR - ===== Failed Load data to the database =====
2025-03-19 15:48:04,284 - ERROR - An error occurred while calling o1906.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 94.0 failed 1 times, most recent failure: Lost task 1.0 in stage 94.0 (TID 118) (pyspark executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '21/5/87/19' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:682)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.time.format.DateTimeParseException: Text '21/5/87/19' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 25 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:65)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '21/5/87/19' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:682)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: java.time.format.DateTimeParseException: Text '21/5/87/19' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 25 more

2025-03-19 15:51:02,571 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 15:51:03,924 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 15:51:05,638 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 15:51:05,850 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 15:51:11,606 - INFO - ===== Start Load data to the database =====
2025-03-19 15:51:12,155 - ERROR - ===== Failed Load data to the database =====
2025-03-19 15:51:12,155 - ERROR - An error occurred while calling o1955.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 97.0 failed 1 times, most recent failure: Lost task 8.0 in stage 97.0 (TID 136) (pyspark executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '1/1/1800' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:682)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.time.format.DateTimeParseException: Text '1/1/1800' could not be parsed, unparsed text found at index 6
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2055)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 25 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:65)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '1/1/1800' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:682)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: java.time.format.DateTimeParseException: Text '1/1/1800' could not be parsed, unparsed text found at index 6
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2055)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 25 more

2025-03-19 15:55:30,317 - INFO - ===== Start Load data to the database =====
2025-03-19 15:55:37,388 - INFO - ===== Finish Load data to the database =====
2025-03-19 15:57:56,567 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 15:57:56,655 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 15:57:58,750 - INFO - ===== Start Load data to the database =====
2025-03-19 15:58:03,927 - INFO - ===== Finish Load data to the database =====
2025-03-19 18:09:40,031 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:09:41,095 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:09:42,296 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:09:42,366 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:10:06,718 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:10:07,674 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:15:18,220 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:15:19,120 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:15:20,913 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:15:20,924 - ERROR - ===== Failed to Transform Data for customers =====
2025-03-19 18:15:20,924 - ERROR - name 'substring' is not defined
2025-03-19 18:16:16,010 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:16:16,852 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:16:17,927 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:16:18,014 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:18:35,216 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:18:36,077 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:18:39,780 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:18:39,877 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:19:00,610 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:19:01,531 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:19:19,486 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:19:19,563 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:23:22,949 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:23:23,824 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:23:27,767 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:23:27,931 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:26:22,402 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:26:23,299 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:26:24,928 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:26:24,995 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:27:35,630 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:27:36,461 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:27:37,515 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:27:37,566 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:29:18,990 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:29:19,970 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:29:20,779 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:29:20,845 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:30:43,584 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:30:44,409 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:30:45,257 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:30:45,318 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:31:02,610 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:31:03,451 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:34:53,434 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:34:54,245 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:34:55,558 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:34:55,581 - ERROR - ===== Failed to Transform Data for customers =====
2025-03-19 18:34:55,582 - ERROR - 'Column' object is not callable
2025-03-19 18:37:12,846 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:37:13,564 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:37:17,070 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:37:17,113 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:38:58,398 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:38:59,327 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:39:00,519 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:39:00,585 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:41:12,101 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:41:12,997 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:41:13,947 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:41:13,990 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:46:56,403 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:46:57,180 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:46:58,150 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:46:58,201 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:50:10,902 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:50:11,813 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:50:13,382 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:50:13,452 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 18:56:13,383 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-19 18:56:14,143 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-19 18:56:16,388 - INFO - ===== Start Transforming Data for customers =====
2025-03-19 18:56:16,484 - INFO - ===== Finished Transforming Data for customers =====
2025-03-19 22:50:39,629 - ERROR - No such comm target registered: jupyter.widget.control
2025-03-19 22:50:39,647 - WARNING - No such comm: de8d1631-b1ee-405a-bf35-1d36adede36e
2025-03-19 23:04:22,669 - ERROR - No such comm target registered: jupyter.widget.control
2025-03-19 23:04:22,681 - WARNING - No such comm: 091ddfa5-38c9-4dd6-8a5f-2c97c9c60884
2025-03-20 01:55:38,189 - ERROR - No such comm target registered: jupyter.widget.control
2025-03-20 01:55:38,200 - WARNING - No such comm: 08cc103a-5a4d-4189-bb32-d4916ec344ec
2025-03-20 03:04:55,820 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:04:58,244 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:04:59,392 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:04:59,642 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:05:39,953 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:05:41,283 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:05:46,016 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:05:46,152 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:07:36,062 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:07:37,316 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:07:38,189 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:07:38,313 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:09:10,125 - INFO - ===== Start Load data to the database =====
2025-03-20 03:09:20,934 - INFO - ===== Finish Load data to the database =====
2025-03-20 03:21:34,522 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:21:36,325 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:21:36,903 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:21:37,069 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:21:41,335 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:21:41,535 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:22:30,368 - INFO - ===== Start Load data to the database =====
2025-03-20 03:22:38,397 - INFO - ===== Finish Load data to the database =====
2025-03-20 03:23:17,524 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:23:18,938 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:23:24,529 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:23:24,664 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:23:31,250 - INFO - ===== Start Load data to the database =====
2025-03-20 03:23:39,928 - INFO - ===== Finish Load data to the database =====
2025-03-20 03:24:06,112 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:24:07,143 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:24:50,347 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:24:50,487 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:25:07,525 - INFO - ===== Start Load data to the database =====
2025-03-20 03:25:13,742 - INFO - ===== Finish Load data to the database =====
2025-03-20 03:25:48,649 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:25:49,734 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:35:43,503 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:35:44,866 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:35:46,285 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:35:46,418 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:35:58,018 - INFO - ===== Start Load data to the database =====
2025-03-20 03:36:06,206 - INFO - ===== Finish Load data to the database =====
2025-03-20 03:36:30,302 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:36:31,527 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:36:32,497 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:36:32,618 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:37:38,849 - INFO - ===== Start Load data to the database =====
2025-03-20 03:37:46,370 - INFO - ===== Finish Load data to the database =====
2025-03-20 03:43:25,173 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:43:26,369 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:43:42,501 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:43:42,648 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:46:29,801 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:46:31,103 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:46:31,903 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:46:32,062 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:47:21,254 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:47:22,415 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:47:24,163 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:47:24,289 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:48:20,744 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:48:21,495 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:48:27,921 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:48:28,042 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:48:54,078 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:48:54,951 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:48:56,664 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:48:56,762 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:50:23,416 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:50:24,351 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:50:26,702 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:50:26,811 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:55:33,531 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:55:34,482 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:55:35,650 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:55:35,762 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 03:56:08,704 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 03:56:09,473 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 03:56:39,988 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 03:56:40,102 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 04:00:04,254 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 04:00:05,134 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 04:00:08,683 - INFO - ===== Start Load data to the database =====
2025-03-20 04:00:15,168 - INFO - ===== Finish Load data to the database =====
2025-03-20 04:14:41,815 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 04:14:42,822 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 04:14:44,680 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 04:14:44,778 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 04:27:08,975 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 04:27:10,245 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 04:27:11,270 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 04:27:11,367 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 04:29:03,432 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 04:29:04,394 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 04:29:07,014 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 04:29:07,110 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 04:30:03,176 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 04:30:04,072 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 04:30:05,174 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 04:30:05,263 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 04:32:39,665 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 04:32:40,527 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 04:32:41,856 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 04:32:41,959 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 04:37:56,736 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 04:37:57,743 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 04:37:58,760 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 04:37:58,867 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 04:40:14,831 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 04:40:15,750 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 04:40:18,225 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 04:40:18,309 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 04:44:19,402 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 04:44:20,284 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 04:44:21,550 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 04:44:21,684 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 04:45:25,268 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 04:45:26,142 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 04:45:27,338 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 04:45:27,425 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 04:49:29,809 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 04:49:30,766 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 04:49:31,741 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 04:49:31,846 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 05:38:32,693 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 05:38:34,945 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 05:38:36,240 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 05:38:36,410 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 05:41:35,659 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 05:41:36,670 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 05:41:39,226 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 05:41:39,323 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 05:45:56,146 - ERROR - No such comm target registered: jupyter.widget.control
2025-03-20 05:45:56,159 - WARNING - No such comm: e8465201-9bb2-4f3e-a8e8-148529698c9c
2025-03-20 06:29:05,775 - ERROR - No such comm target registered: jupyter.widget.control
2025-03-20 06:29:05,807 - WARNING - No such comm: 55a1ef9d-b491-4778-aba7-350a107bfec7
2025-03-20 06:30:17,061 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 06:30:18,510 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 06:30:19,626 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:30:19,750 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 06:31:25,740 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 06:31:27,042 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 06:31:28,301 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:31:28,439 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 06:33:20,615 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 06:33:22,012 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 06:33:23,367 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:33:23,446 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 06:33:55,492 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 06:33:56,721 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 06:33:57,687 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:33:57,766 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 06:34:45,631 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 06:34:46,964 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 06:34:47,873 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:34:47,942 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 06:44:34,351 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 06:44:35,778 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 06:44:44,552 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:44:44,893 - ERROR - ===== Failed to Transform Data for customers =====
2025-03-20 06:44:44,894 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `CustomerDOB` cannot be resolved. Did you mean one of the following? [`customer_id`, `customer_gender`, `birth_date`, `customer_location`, `TransactionID`].;
'Project [TransactionID#14975, customer_id#14993, CASE WHEN (cast(regexp_extract('CustomerDOB, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract('CustomerDOB, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract('CustomerDOB, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract('CustomerDOB, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date('CustomerDOB, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#15044, customer_gender#15014, customer_location#15024, account_balance#15034, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
+- Project [TransactionID#14975, customer_id#14993, birth_date#15004, customer_gender#15014, customer_location#15024, CustAccountBalance#14980 AS account_balance#15034, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
   +- Project [TransactionID#14975, customer_id#14993, birth_date#15004, customer_gender#15014, CustLocation#14979 AS customer_location#15024, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
      +- Project [TransactionID#14975, customer_id#14993, birth_date#15004, CustGender#14978 AS customer_gender#15014, CustLocation#14979, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
         +- Project [TransactionID#14975, customer_id#14993, CustomerDOB#14977 AS birth_date#15004, CustGender#14978, CustLocation#14979, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
            +- Project [TransactionID#14975, CustomerID#14976 AS customer_id#14993, CustomerDOB#14977, CustGender#14978, CustLocation#14979, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
               +- Relation [TransactionID#14975,CustomerID#14976,CustomerDOB#14977,CustGender#14978,CustLocation#14979,CustAccountBalance#14980,TransactionDate#14981,TransactionTime#14982,TransactionAmount (INR)#14983] csv

2025-03-20 06:50:32,215 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:50:32,360 - ERROR - ===== Failed to Transform Data for customers =====
2025-03-20 06:50:32,361 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `CustomerDOB` cannot be resolved. Did you mean one of the following? [`customer_id`, `customer_gender`, `birth_date`, `customer_location`, `TransactionID`].;
'Project [TransactionID#14975, customer_id#15091, CASE WHEN (cast(regexp_extract('CustomerDOB, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract('CustomerDOB, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract('CustomerDOB, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract('CustomerDOB, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date('CustomerDOB, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#15141, customer_gender#15111, customer_location#15121, account_balance#15131, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
+- Project [TransactionID#14975, customer_id#15091, birth_date#15101, customer_gender#15111, customer_location#15121, CustAccountBalance#14980 AS account_balance#15131, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
   +- Project [TransactionID#14975, customer_id#15091, birth_date#15101, customer_gender#15111, CustLocation#14979 AS customer_location#15121, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
      +- Project [TransactionID#14975, customer_id#15091, birth_date#15101, CustGender#14978 AS customer_gender#15111, CustLocation#14979, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
         +- Project [TransactionID#14975, customer_id#15091, CustomerDOB#14977 AS birth_date#15101, CustGender#14978, CustLocation#14979, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
            +- Project [TransactionID#14975, CustomerID#14976 AS customer_id#15091, CustomerDOB#14977, CustGender#14978, CustLocation#14979, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
               +- Relation [TransactionID#14975,CustomerID#14976,CustomerDOB#14977,CustGender#14978,CustLocation#14979,CustAccountBalance#14980,TransactionDate#14981,TransactionTime#14982,TransactionAmount (INR)#14983] csv

2025-03-20 06:52:10,731 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:52:10,883 - ERROR - ===== Failed to Transform Data for customers =====
2025-03-20 06:52:10,884 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `CustomerDOB` cannot be resolved. Did you mean one of the following? [`customer_id`, `customer_gender`, `birth_date`, `customer_location`, `TransactionID`].;
'Project [TransactionID#14975, customer_id#15142, CASE WHEN (cast(regexp_extract(birth_date#15152, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract('CustomerDOB, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract('CustomerDOB, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract('CustomerDOB, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date(birth_date#15152, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#15192, customer_gender#15162, customer_location#15172, account_balance#15182, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
+- Project [TransactionID#14975, customer_id#15142, birth_date#15152, customer_gender#15162, customer_location#15172, CustAccountBalance#14980 AS account_balance#15182, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
   +- Project [TransactionID#14975, customer_id#15142, birth_date#15152, customer_gender#15162, CustLocation#14979 AS customer_location#15172, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
      +- Project [TransactionID#14975, customer_id#15142, birth_date#15152, CustGender#14978 AS customer_gender#15162, CustLocation#14979, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
         +- Project [TransactionID#14975, customer_id#15142, CustomerDOB#14977 AS birth_date#15152, CustGender#14978, CustLocation#14979, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
            +- Project [TransactionID#14975, CustomerID#14976 AS customer_id#15142, CustomerDOB#14977, CustGender#14978, CustLocation#14979, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
               +- Relation [TransactionID#14975,CustomerID#14976,CustomerDOB#14977,CustGender#14978,CustLocation#14979,CustAccountBalance#14980,TransactionDate#14981,TransactionTime#14982,TransactionAmount (INR)#14983] csv

2025-03-20 06:52:47,054 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:52:47,201 - ERROR - ===== Failed to Transform Data for customers =====
2025-03-20 06:52:47,202 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `CustGender` cannot be resolved. Did you mean one of the following? [`customer_gender`, `customer_id`, `birth_date`, `TransactionID`, `TransactionDate`].;
'Project [TransactionID#14975, customer_id#15193, birth_date#15243, lower(trim('CustGender, None)) AS customer_gender#15263, customer_location#15223, account_balance#15253, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
+- Project [TransactionID#14975, customer_id#15193, birth_date#15243, customer_gender#15213, customer_location#15223, cast(account_balance#15233 as double) AS account_balance#15253, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
   +- Project [TransactionID#14975, customer_id#15193, CASE WHEN (cast(regexp_extract(birth_date#15203, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract(birth_date#15203, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract(birth_date#15203, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract(birth_date#15203, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date(birth_date#15203, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#15243, customer_gender#15213, customer_location#15223, account_balance#15233, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
      +- Project [TransactionID#14975, customer_id#15193, birth_date#15203, customer_gender#15213, customer_location#15223, CustAccountBalance#14980 AS account_balance#15233, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
         +- Project [TransactionID#14975, customer_id#15193, birth_date#15203, customer_gender#15213, CustLocation#14979 AS customer_location#15223, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
            +- Project [TransactionID#14975, customer_id#15193, birth_date#15203, CustGender#14978 AS customer_gender#15213, CustLocation#14979, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
               +- Project [TransactionID#14975, customer_id#15193, CustomerDOB#14977 AS birth_date#15203, CustGender#14978, CustLocation#14979, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
                  +- Project [TransactionID#14975, CustomerID#14976 AS customer_id#15193, CustomerDOB#14977, CustGender#14978, CustLocation#14979, CustAccountBalance#14980, TransactionDate#14981, TransactionTime#14982, TransactionAmount (INR)#14983]
                     +- Relation [TransactionID#14975,CustomerID#14976,CustomerDOB#14977,CustGender#14978,CustLocation#14979,CustAccountBalance#14980,TransactionDate#14981,TransactionTime#14982,TransactionAmount (INR)#14983] csv

2025-03-20 06:53:48,441 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:53:48,546 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 06:55:01,484 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 06:55:02,501 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 06:55:03,703 - INFO - ===== Start Transforming Data for transaction =====
2025-03-20 06:55:03,705 - INFO - ===== Finished Transforming Data for transaction =====
2025-03-20 06:56:01,510 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:56:01,593 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 06:56:04,376 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 06:56:05,246 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 06:56:06,357 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:56:06,461 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 06:59:38,177 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 06:59:39,287 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 06:59:40,361 - INFO - ===== Start Transforming Data for customers =====
2025-03-20 06:59:40,458 - INFO - ===== Finished Transforming Data for customers =====
2025-03-20 07:00:16,792 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 07:00:17,956 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 07:00:18,804 - INFO - ===== Start Transforming Data for transaction =====
2025-03-20 07:00:18,832 - INFO - ===== Finished Transforming Data for transaction =====
2025-03-20 07:03:01,182 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 07:03:02,462 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 07:03:03,465 - INFO - ===== Start Transforming Data for transactions =====
2025-03-20 07:03:03,599 - ERROR - ===== Failed to Transform Data for transactions =====
2025-03-20 07:03:03,600 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `TransactionAmount (INR)` cannot be resolved. Did you mean one of the following? [`transaction_amount`, `transaction_date`, `transaction_time`, `transaction_id`, `CustAccountBalance`].;
'Project [transaction_id#16169, customer_id#16180, CustomerDOB#16153, CustGender#16154, CustLocation#16155, CustAccountBalance#16156, transaction_date#16220, transaction_time#16241, cast('TransactionAmount (INR) as double) AS transaction_amount#16271]
+- Project [transaction_id#16169, customer_id#16180, CustomerDOB#16153, CustGender#16154, CustLocation#16155, CustAccountBalance#16156, transaction_date#16220, transaction_time#16241, cast(transaction_amount#16210 as double) AS transaction_amount#16261]
   +- Project [transaction_id#16169, customer_id#16180, CustomerDOB#16153, CustGender#16154, CustLocation#16155, CustAccountBalance#16156, transaction_date#16220, transaction_time#16241, transaction_amount#16210]
      +- Project [transaction_id#16169, customer_id#16180, CustomerDOB#16153, CustGender#16154, CustLocation#16155, CustAccountBalance#16156, transaction_date#16220, concat(substring(padded_time#16230, 1, 2), :, substring(padded_time#16230, 3, 2), :, substring(padded_time#16230, 5, 2)) AS transaction_time#16241, transaction_amount#16210, padded_time#16230]
         +- Project [transaction_id#16169, customer_id#16180, CustomerDOB#16153, CustGender#16154, CustLocation#16155, CustAccountBalance#16156, transaction_date#16220, transaction_time#16200, transaction_amount#16210, lpad(cast(transaction_date#16220 as string), 6, 0) AS padded_time#16230]
            +- Project [transaction_id#16169, customer_id#16180, CustomerDOB#16153, CustGender#16154, CustLocation#16155, CustAccountBalance#16156, to_date(transaction_date#16190, Some(d/M/yy), Some(Etc/UTC), false) AS transaction_date#16220, transaction_time#16200, transaction_amount#16210]
               +- Project [transaction_id#16169, customer_id#16180, CustomerDOB#16153, CustGender#16154, CustLocation#16155, CustAccountBalance#16156, transaction_date#16190, transaction_time#16200, TransactionAmount (INR)#16159 AS transaction_amount#16210]
                  +- Project [transaction_id#16169, customer_id#16180, CustomerDOB#16153, CustGender#16154, CustLocation#16155, CustAccountBalance#16156, transaction_date#16190, TransactionTime#16158 AS transaction_time#16200, TransactionAmount (INR)#16159]
                     +- Project [transaction_id#16169, customer_id#16180, CustomerDOB#16153, CustGender#16154, CustLocation#16155, CustAccountBalance#16156, TransactionDate#16157 AS transaction_date#16190, TransactionTime#16158, TransactionAmount (INR)#16159]
                        +- Project [transaction_id#16169, CustomerID#16152 AS customer_id#16180, CustomerDOB#16153, CustGender#16154, CustLocation#16155, CustAccountBalance#16156, TransactionDate#16157, TransactionTime#16158, TransactionAmount (INR)#16159]
                           +- Project [TransactionID#16151 AS transaction_id#16169, CustomerID#16152, CustomerDOB#16153, CustGender#16154, CustLocation#16155, CustAccountBalance#16156, TransactionDate#16157, TransactionTime#16158, TransactionAmount (INR)#16159]
                              +- Relation [TransactionID#16151,CustomerID#16152,CustomerDOB#16153,CustGender#16154,CustLocation#16155,CustAccountBalance#16156,TransactionDate#16157,TransactionTime#16158,TransactionAmount (INR)#16159] csv

2025-03-20 07:04:50,882 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 07:04:52,060 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 07:04:52,975 - INFO - ===== Start Transforming Data for transactions =====
2025-03-20 07:04:53,053 - INFO - ===== Finished Transforming Data for transactions =====
2025-03-20 07:05:33,184 - INFO - ===== Start Load data to the database =====
2025-03-20 07:05:42,650 - INFO - ===== Finish Load data to the database =====
2025-03-20 07:06:01,943 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 07:06:03,052 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 07:16:21,414 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 07:16:22,531 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 07:16:23,504 - INFO - ===== Start Transforming Data for transactions =====
2025-03-20 07:16:23,575 - INFO - ===== Finished Transforming Data for transactions =====
2025-03-20 07:16:49,232 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-20 07:16:50,663 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-20 07:19:14,235 - ERROR - No such comm target registered: jupyter.widget.control
2025-03-20 07:19:14,240 - WARNING - No such comm: 4827964b-84cb-42f2-b89c-dceb1481446c
2025-03-20 22:09:35,161 - ERROR - No such comm target registered: jupyter.widget.control
2025-03-20 22:09:35,175 - WARNING - No such comm: 1063a04a-9cc0-44ab-a3ed-ce9e17284f4d
2025-03-21 17:46:42,287 - ERROR - No such comm target registered: jupyter.widget.control
2025-03-21 17:46:42,339 - WARNING - No such comm: 64b07bc1-522d-462d-bcd0-65c4e32c0503
2025-03-21 18:22:48,213 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 18:22:50,585 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 18:22:58,294 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 18:22:58,389 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 18:23:06,077 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 18:23:06,139 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 18:23:18,703 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 18:23:18,792 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 18:23:36,312 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 18:23:36,881 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 18:23:38,221 - INFO - ===== Start Transforming Data for transactions =====
2025-03-21 18:23:38,288 - INFO - ===== Finished Transforming Data for transactions =====
2025-03-21 18:24:33,633 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 18:24:34,190 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 18:24:36,477 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 18:24:36,525 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 18:24:42,448 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 18:24:43,048 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 18:24:44,990 - INFO - ===== Start Transforming Data for transactions =====
2025-03-21 18:24:45,038 - INFO - ===== Finished Transforming Data for transactions =====
2025-03-21 18:36:45,949 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 18:36:46,711 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 18:36:48,254 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 18:36:48,297 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 18:36:48,322 - ERROR - ===== Failed Selecting Data process for table customers =====
2025-03-21 18:36:48,323 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `gender` cannot be resolved. Did you mean one of the following? [`birth_date`, `customer_gender`, `customer_id`, `TransactionID`, `TransactionDate`].;
'Project [customer_id#17929, birth_date#17980, 'gender, 'location, account_balance#17990, 'created_at, 'updated_at]
+- Project [TransactionID#17911, customer_id#17929, birth_date#17980, customer_gender#17950, customer_location#17960, cast(account_balance#17970 as double) AS account_balance#17990, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
   +- Project [TransactionID#17911, customer_id#17929, CASE WHEN (cast(regexp_extract(birth_date#17940, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract(birth_date#17940, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract(birth_date#17940, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract(birth_date#17940, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date(birth_date#17940, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#17980, customer_gender#17950, customer_location#17960, account_balance#17970, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
      +- Project [TransactionID#17911, customer_id#17929, birth_date#17940, customer_gender#17950, customer_location#17960, CustAccountBalance#17916 AS account_balance#17970, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
         +- Project [TransactionID#17911, customer_id#17929, birth_date#17940, customer_gender#17950, CustLocation#17915 AS customer_location#17960, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
            +- Project [TransactionID#17911, customer_id#17929, birth_date#17940, CustGender#17914 AS customer_gender#17950, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
               +- Project [TransactionID#17911, customer_id#17929, CustomerDOB#17913 AS birth_date#17940, CustGender#17914, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
                  +- Project [TransactionID#17911, CustomerID#17912 AS customer_id#17929, CustomerDOB#17913, CustGender#17914, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
                     +- Relation [TransactionID#17911,CustomerID#17912,CustomerDOB#17913,CustGender#17914,CustLocation#17915,CustAccountBalance#17916,TransactionDate#17917,TransactionTime#17918,TransactionAmount (INR)#17919] csv

2025-03-21 18:36:48,332 - ERROR - ===== Failed to Transform Data for customers =====
2025-03-21 18:36:48,333 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `gender` cannot be resolved. Did you mean one of the following? [`birth_date`, `customer_gender`, `customer_id`, `TransactionID`, `TransactionDate`].;
'Project [customer_id#17929, birth_date#17980, 'gender, 'location, account_balance#17990, 'created_at, 'updated_at]
+- Project [TransactionID#17911, customer_id#17929, birth_date#17980, customer_gender#17950, customer_location#17960, cast(account_balance#17970 as double) AS account_balance#17990, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
   +- Project [TransactionID#17911, customer_id#17929, CASE WHEN (cast(regexp_extract(birth_date#17940, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract(birth_date#17940, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract(birth_date#17940, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract(birth_date#17940, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date(birth_date#17940, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#17980, customer_gender#17950, customer_location#17960, account_balance#17970, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
      +- Project [TransactionID#17911, customer_id#17929, birth_date#17940, customer_gender#17950, customer_location#17960, CustAccountBalance#17916 AS account_balance#17970, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
         +- Project [TransactionID#17911, customer_id#17929, birth_date#17940, customer_gender#17950, CustLocation#17915 AS customer_location#17960, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
            +- Project [TransactionID#17911, customer_id#17929, birth_date#17940, CustGender#17914 AS customer_gender#17950, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
               +- Project [TransactionID#17911, customer_id#17929, CustomerDOB#17913 AS birth_date#17940, CustGender#17914, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
                  +- Project [TransactionID#17911, CustomerID#17912 AS customer_id#17929, CustomerDOB#17913, CustGender#17914, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
                     +- Relation [TransactionID#17911,CustomerID#17912,CustomerDOB#17913,CustGender#17914,CustLocation#17915,CustAccountBalance#17916,TransactionDate#17917,TransactionTime#17918,TransactionAmount (INR)#17919] csv

2025-03-21 18:37:39,829 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 18:37:39,870 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 18:37:39,888 - ERROR - ===== Failed Selecting Data process for table customers =====
2025-03-21 18:37:39,888 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `location` cannot be resolved. Did you mean one of the following? [`gender`, `TransactionID`, `birth_date`, `customer_id`, `customer_location`].;
'Project [customer_id#18000, birth_date#18050, gender#18020, 'location, account_balance#18060, 'created_at, 'updated_at]
+- Project [TransactionID#17911, customer_id#18000, birth_date#18050, gender#18020, customer_location#18030, cast(account_balance#18040 as double) AS account_balance#18060, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
   +- Project [TransactionID#17911, customer_id#18000, CASE WHEN (cast(regexp_extract(birth_date#18010, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract(birth_date#18010, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract(birth_date#18010, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract(birth_date#18010, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date(birth_date#18010, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#18050, gender#18020, customer_location#18030, account_balance#18040, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
      +- Project [TransactionID#17911, customer_id#18000, birth_date#18010, gender#18020, customer_location#18030, CustAccountBalance#17916 AS account_balance#18040, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
         +- Project [TransactionID#17911, customer_id#18000, birth_date#18010, gender#18020, CustLocation#17915 AS customer_location#18030, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
            +- Project [TransactionID#17911, customer_id#18000, birth_date#18010, CustGender#17914 AS gender#18020, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
               +- Project [TransactionID#17911, customer_id#18000, CustomerDOB#17913 AS birth_date#18010, CustGender#17914, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
                  +- Project [TransactionID#17911, CustomerID#17912 AS customer_id#18000, CustomerDOB#17913, CustGender#17914, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
                     +- Relation [TransactionID#17911,CustomerID#17912,CustomerDOB#17913,CustGender#17914,CustLocation#17915,CustAccountBalance#17916,TransactionDate#17917,TransactionTime#17918,TransactionAmount (INR)#17919] csv

2025-03-21 18:37:39,894 - ERROR - ===== Failed to Transform Data for customers =====
2025-03-21 18:37:39,895 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `location` cannot be resolved. Did you mean one of the following? [`gender`, `TransactionID`, `birth_date`, `customer_id`, `customer_location`].;
'Project [customer_id#18000, birth_date#18050, gender#18020, 'location, account_balance#18060, 'created_at, 'updated_at]
+- Project [TransactionID#17911, customer_id#18000, birth_date#18050, gender#18020, customer_location#18030, cast(account_balance#18040 as double) AS account_balance#18060, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
   +- Project [TransactionID#17911, customer_id#18000, CASE WHEN (cast(regexp_extract(birth_date#18010, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract(birth_date#18010, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract(birth_date#18010, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract(birth_date#18010, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date(birth_date#18010, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#18050, gender#18020, customer_location#18030, account_balance#18040, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
      +- Project [TransactionID#17911, customer_id#18000, birth_date#18010, gender#18020, customer_location#18030, CustAccountBalance#17916 AS account_balance#18040, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
         +- Project [TransactionID#17911, customer_id#18000, birth_date#18010, gender#18020, CustLocation#17915 AS customer_location#18030, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
            +- Project [TransactionID#17911, customer_id#18000, birth_date#18010, CustGender#17914 AS gender#18020, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
               +- Project [TransactionID#17911, customer_id#18000, CustomerDOB#17913 AS birth_date#18010, CustGender#17914, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
                  +- Project [TransactionID#17911, CustomerID#17912 AS customer_id#18000, CustomerDOB#17913, CustGender#17914, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
                     +- Relation [TransactionID#17911,CustomerID#17912,CustomerDOB#17913,CustGender#17914,CustLocation#17915,CustAccountBalance#17916,TransactionDate#17917,TransactionTime#17918,TransactionAmount (INR)#17919] csv

2025-03-21 18:38:42,728 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 18:38:42,774 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 18:38:42,800 - ERROR - ===== Failed Selecting Data process for table customers =====
2025-03-21 18:38:42,801 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `created_at` cannot be resolved. Did you mean one of the following? [`gender`, `birth_date`, `location`, `customer_id`, `TransactionDate`].;
'Project [customer_id#18070, birth_date#18120, gender#18090, location#18100, account_balance#18130, 'created_at, 'updated_at]
+- Project [TransactionID#17911, customer_id#18070, birth_date#18120, gender#18090, location#18100, cast(account_balance#18110 as double) AS account_balance#18130, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
   +- Project [TransactionID#17911, customer_id#18070, CASE WHEN (cast(regexp_extract(birth_date#18080, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract(birth_date#18080, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract(birth_date#18080, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract(birth_date#18080, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date(birth_date#18080, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#18120, gender#18090, location#18100, account_balance#18110, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
      +- Project [TransactionID#17911, customer_id#18070, birth_date#18080, gender#18090, location#18100, CustAccountBalance#17916 AS account_balance#18110, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
         +- Project [TransactionID#17911, customer_id#18070, birth_date#18080, gender#18090, CustLocation#17915 AS location#18100, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
            +- Project [TransactionID#17911, customer_id#18070, birth_date#18080, CustGender#17914 AS gender#18090, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
               +- Project [TransactionID#17911, customer_id#18070, CustomerDOB#17913 AS birth_date#18080, CustGender#17914, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
                  +- Project [TransactionID#17911, CustomerID#17912 AS customer_id#18070, CustomerDOB#17913, CustGender#17914, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
                     +- Relation [TransactionID#17911,CustomerID#17912,CustomerDOB#17913,CustGender#17914,CustLocation#17915,CustAccountBalance#17916,TransactionDate#17917,TransactionTime#17918,TransactionAmount (INR)#17919] csv

2025-03-21 18:38:42,803 - ERROR - ===== Failed to Transform Data for customers =====
2025-03-21 18:38:42,804 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `created_at` cannot be resolved. Did you mean one of the following? [`gender`, `birth_date`, `location`, `customer_id`, `TransactionDate`].;
'Project [customer_id#18070, birth_date#18120, gender#18090, location#18100, account_balance#18130, 'created_at, 'updated_at]
+- Project [TransactionID#17911, customer_id#18070, birth_date#18120, gender#18090, location#18100, cast(account_balance#18110 as double) AS account_balance#18130, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
   +- Project [TransactionID#17911, customer_id#18070, CASE WHEN (cast(regexp_extract(birth_date#18080, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract(birth_date#18080, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract(birth_date#18080, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract(birth_date#18080, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date(birth_date#18080, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#18120, gender#18090, location#18100, account_balance#18110, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
      +- Project [TransactionID#17911, customer_id#18070, birth_date#18080, gender#18090, location#18100, CustAccountBalance#17916 AS account_balance#18110, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
         +- Project [TransactionID#17911, customer_id#18070, birth_date#18080, gender#18090, CustLocation#17915 AS location#18100, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
            +- Project [TransactionID#17911, customer_id#18070, birth_date#18080, CustGender#17914 AS gender#18090, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
               +- Project [TransactionID#17911, customer_id#18070, CustomerDOB#17913 AS birth_date#18080, CustGender#17914, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
                  +- Project [TransactionID#17911, CustomerID#17912 AS customer_id#18070, CustomerDOB#17913, CustGender#17914, CustLocation#17915, CustAccountBalance#17916, TransactionDate#17917, TransactionTime#17918, TransactionAmount (INR)#17919]
                     +- Relation [TransactionID#17911,CustomerID#17912,CustomerDOB#17913,CustGender#17914,CustLocation#17915,CustAccountBalance#17916,TransactionDate#17917,TransactionTime#17918,TransactionAmount (INR)#17919] csv

2025-03-21 18:39:40,366 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 18:39:40,405 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 18:39:40,412 - INFO - ===== Finish Selecting Data process for table customers =====
2025-03-21 18:39:40,427 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 18:40:06,740 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 18:40:07,336 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 18:40:08,963 - INFO - ===== Start Transforming Data for transactions =====
2025-03-21 18:40:09,004 - INFO - ===== Start Selecting Data process for table transactions =====
2025-03-21 18:40:09,014 - INFO - ===== Finish Selecting Data process for table transactions =====
2025-03-21 18:40:09,015 - INFO - ===== Finished Transforming Data for transactions =====
2025-03-21 22:23:52,850 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 22:23:55,427 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 22:23:55,923 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 22:23:56,038 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 22:23:56,070 - INFO - ===== Finish Selecting Data process for table customers =====
2025-03-21 22:23:56,109 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 22:23:59,469 - INFO - ===== Start Load data to the database =====
2025-03-21 22:23:59,816 - ERROR - ===== Failed Load data to the database =====
2025-03-21 22:23:59,817 - ERROR - An error occurred while calling o6308.load.
: org.postgresql.util.PSQLException: ERROR: syntax error at or near "TABLE"
  Position: 25
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:368)
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:498)
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:415)
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:190)
	at org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:134)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:68)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)

2025-03-21 22:27:59,662 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:28:00,031 - INFO - ===== Truncated table customers successfully =====
2025-03-21 22:28:00,265 - ERROR - ===== Failed Load Data to the Database for table customers =====
2025-03-21 22:28:00,266 - ERROR - Column customer_id not found in schema Some(StructType(StructField(TransactionID,StringType,true),StructField(CustomerID,StringType,true),StructField(CustomerDOB,StringType,true),StructField(CustGender,StringType,true),StructField(CustLocation,StringType,true),StructField(CustAccountBalance,StringType,true),StructField(TransactionDate,StringType,true),StructField(TransactionTime,StringType,true),StructField(TransactionAmount (INR),StringType,true))).
2025-03-21 22:29:54,220 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 22:29:56,000 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 22:29:57,022 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 22:29:57,085 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 22:29:57,124 - ERROR - ===== Failed Selecting Data process for table customers =====
2025-03-21 22:29:57,125 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_id` cannot be resolved. Did you mean one of the following? [`CustomerID`, `gender`, `birth_date`, `location`, `TransactionDate`].;
'Project ['customer_id, birth_date#18683, gender#18653, location#18663, account_balance#18693]
+- Project [TransactionID#18624, CustomerID#18625, birth_date#18683, gender#18653, location#18663, cast(account_balance#18673 as double) AS account_balance#18693, TransactionDate#18630, TransactionTime#18631, TransactionAmount (INR)#18632]
   +- Project [TransactionID#18624, CustomerID#18625, CASE WHEN (cast(regexp_extract(birth_date#18642, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract(birth_date#18642, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract(birth_date#18642, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract(birth_date#18642, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date(birth_date#18642, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#18683, gender#18653, location#18663, account_balance#18673, TransactionDate#18630, TransactionTime#18631, TransactionAmount (INR)#18632]
      +- Project [TransactionID#18624, CustomerID#18625, birth_date#18642, gender#18653, location#18663, CustAccountBalance#18629 AS account_balance#18673, TransactionDate#18630, TransactionTime#18631, TransactionAmount (INR)#18632]
         +- Project [TransactionID#18624, CustomerID#18625, birth_date#18642, gender#18653, CustLocation#18628 AS location#18663, CustAccountBalance#18629, TransactionDate#18630, TransactionTime#18631, TransactionAmount (INR)#18632]
            +- Project [TransactionID#18624, CustomerID#18625, birth_date#18642, CustGender#18627 AS gender#18653, CustLocation#18628, CustAccountBalance#18629, TransactionDate#18630, TransactionTime#18631, TransactionAmount (INR)#18632]
               +- Project [TransactionID#18624, CustomerID#18625, CustomerDOB#18626 AS birth_date#18642, CustGender#18627, CustLocation#18628, CustAccountBalance#18629, TransactionDate#18630, TransactionTime#18631, TransactionAmount (INR)#18632]
                  +- Relation [TransactionID#18624,CustomerID#18625,CustomerDOB#18626,CustGender#18627,CustLocation#18628,CustAccountBalance#18629,TransactionDate#18630,TransactionTime#18631,TransactionAmount (INR)#18632] csv

2025-03-21 22:29:57,134 - ERROR - ===== Failed to Transform Data for customers =====
2025-03-21 22:29:57,135 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_id` cannot be resolved. Did you mean one of the following? [`CustomerID`, `gender`, `birth_date`, `location`, `TransactionDate`].;
'Project ['customer_id, birth_date#18683, gender#18653, location#18663, account_balance#18693]
+- Project [TransactionID#18624, CustomerID#18625, birth_date#18683, gender#18653, location#18663, cast(account_balance#18673 as double) AS account_balance#18693, TransactionDate#18630, TransactionTime#18631, TransactionAmount (INR)#18632]
   +- Project [TransactionID#18624, CustomerID#18625, CASE WHEN (cast(regexp_extract(birth_date#18642, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract(birth_date#18642, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract(birth_date#18642, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract(birth_date#18642, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date(birth_date#18642, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#18683, gender#18653, location#18663, account_balance#18673, TransactionDate#18630, TransactionTime#18631, TransactionAmount (INR)#18632]
      +- Project [TransactionID#18624, CustomerID#18625, birth_date#18642, gender#18653, location#18663, CustAccountBalance#18629 AS account_balance#18673, TransactionDate#18630, TransactionTime#18631, TransactionAmount (INR)#18632]
         +- Project [TransactionID#18624, CustomerID#18625, birth_date#18642, gender#18653, CustLocation#18628 AS location#18663, CustAccountBalance#18629, TransactionDate#18630, TransactionTime#18631, TransactionAmount (INR)#18632]
            +- Project [TransactionID#18624, CustomerID#18625, birth_date#18642, CustGender#18627 AS gender#18653, CustLocation#18628, CustAccountBalance#18629, TransactionDate#18630, TransactionTime#18631, TransactionAmount (INR)#18632]
               +- Project [TransactionID#18624, CustomerID#18625, CustomerDOB#18626 AS birth_date#18642, CustGender#18627, CustLocation#18628, CustAccountBalance#18629, TransactionDate#18630, TransactionTime#18631, TransactionAmount (INR)#18632]
                  +- Relation [TransactionID#18624,CustomerID#18625,CustomerDOB#18626,CustGender#18627,CustLocation#18628,CustAccountBalance#18629,TransactionDate#18630,TransactionTime#18631,TransactionAmount (INR)#18632] csv

2025-03-21 22:30:24,831 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 22:30:25,951 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 22:30:27,210 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 22:30:27,266 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 22:30:27,293 - ERROR - ===== Failed Selecting Data process for table customers =====
2025-03-21 22:30:27,294 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_id` cannot be resolved. Did you mean one of the following? [`CustomerID`, `gender`, `birth_date`, `location`, `TransactionDate`].;
'Project ['customer_id, birth_date#18779, gender#18749, location#18759, account_balance#18789]
+- Project [TransactionID#18720, CustomerID#18721, birth_date#18779, gender#18749, location#18759, cast(account_balance#18769 as double) AS account_balance#18789, TransactionDate#18726, TransactionTime#18727, TransactionAmount (INR)#18728]
   +- Project [TransactionID#18720, CustomerID#18721, CASE WHEN (cast(regexp_extract(birth_date#18738, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract(birth_date#18738, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract(birth_date#18738, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract(birth_date#18738, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date(birth_date#18738, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#18779, gender#18749, location#18759, account_balance#18769, TransactionDate#18726, TransactionTime#18727, TransactionAmount (INR)#18728]
      +- Project [TransactionID#18720, CustomerID#18721, birth_date#18738, gender#18749, location#18759, CustAccountBalance#18725 AS account_balance#18769, TransactionDate#18726, TransactionTime#18727, TransactionAmount (INR)#18728]
         +- Project [TransactionID#18720, CustomerID#18721, birth_date#18738, gender#18749, CustLocation#18724 AS location#18759, CustAccountBalance#18725, TransactionDate#18726, TransactionTime#18727, TransactionAmount (INR)#18728]
            +- Project [TransactionID#18720, CustomerID#18721, birth_date#18738, CustGender#18723 AS gender#18749, CustLocation#18724, CustAccountBalance#18725, TransactionDate#18726, TransactionTime#18727, TransactionAmount (INR)#18728]
               +- Project [TransactionID#18720, CustomerID#18721, CustomerDOB#18722 AS birth_date#18738, CustGender#18723, CustLocation#18724, CustAccountBalance#18725, TransactionDate#18726, TransactionTime#18727, TransactionAmount (INR)#18728]
                  +- Relation [TransactionID#18720,CustomerID#18721,CustomerDOB#18722,CustGender#18723,CustLocation#18724,CustAccountBalance#18725,TransactionDate#18726,TransactionTime#18727,TransactionAmount (INR)#18728] csv

2025-03-21 22:30:27,298 - ERROR - ===== Failed to Transform Data for customers =====
2025-03-21 22:30:27,299 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_id` cannot be resolved. Did you mean one of the following? [`CustomerID`, `gender`, `birth_date`, `location`, `TransactionDate`].;
'Project ['customer_id, birth_date#18779, gender#18749, location#18759, account_balance#18789]
+- Project [TransactionID#18720, CustomerID#18721, birth_date#18779, gender#18749, location#18759, cast(account_balance#18769 as double) AS account_balance#18789, TransactionDate#18726, TransactionTime#18727, TransactionAmount (INR)#18728]
   +- Project [TransactionID#18720, CustomerID#18721, CASE WHEN (cast(regexp_extract(birth_date#18738, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract(birth_date#18738, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract(birth_date#18738, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract(birth_date#18738, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date(birth_date#18738, Some(d/M/yy), Some(Etc/UTC), false) END AS birth_date#18779, gender#18749, location#18759, account_balance#18769, TransactionDate#18726, TransactionTime#18727, TransactionAmount (INR)#18728]
      +- Project [TransactionID#18720, CustomerID#18721, birth_date#18738, gender#18749, location#18759, CustAccountBalance#18725 AS account_balance#18769, TransactionDate#18726, TransactionTime#18727, TransactionAmount (INR)#18728]
         +- Project [TransactionID#18720, CustomerID#18721, birth_date#18738, gender#18749, CustLocation#18724 AS location#18759, CustAccountBalance#18725, TransactionDate#18726, TransactionTime#18727, TransactionAmount (INR)#18728]
            +- Project [TransactionID#18720, CustomerID#18721, birth_date#18738, CustGender#18723 AS gender#18749, CustLocation#18724, CustAccountBalance#18725, TransactionDate#18726, TransactionTime#18727, TransactionAmount (INR)#18728]
               +- Project [TransactionID#18720, CustomerID#18721, CustomerDOB#18722 AS birth_date#18738, CustGender#18723, CustLocation#18724, CustAccountBalance#18725, TransactionDate#18726, TransactionTime#18727, TransactionAmount (INR)#18728]
                  +- Relation [TransactionID#18720,CustomerID#18721,CustomerDOB#18722,CustGender#18723,CustLocation#18724,CustAccountBalance#18725,TransactionDate#18726,TransactionTime#18727,TransactionAmount (INR)#18728] csv

2025-03-21 22:31:00,277 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 22:31:00,344 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 22:31:00,361 - INFO - ===== Finish Selecting Data process for table customers =====
2025-03-21 22:31:00,381 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 22:31:02,463 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:31:02,508 - INFO - ===== Truncated table customers successfully =====
2025-03-21 22:31:02,552 - ERROR - ===== Failed Load Data to the Database for table customers =====
2025-03-21 22:31:02,552 - ERROR - Column birth_date not found in schema Some(StructType(StructField(TransactionID,StringType,true),StructField(CustomerID,StringType,true),StructField(CustomerDOB,StringType,true),StructField(CustGender,StringType,true),StructField(CustLocation,StringType,true),StructField(CustAccountBalance,StringType,true),StructField(TransactionDate,StringType,true),StructField(TransactionTime,StringType,true),StructField(TransactionAmount (INR),StringType,true))).
2025-03-21 22:32:12,974 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 22:32:14,241 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 22:32:15,103 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 22:32:15,157 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 22:32:15,173 - INFO - ===== Finish Selecting Data process for table customers =====
2025-03-21 22:32:15,191 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 22:32:16,724 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:32:16,761 - INFO - ===== Truncated table customers successfully =====
2025-03-21 22:32:16,806 - ERROR - ===== Failed Load Data to the Database for table customers =====
2025-03-21 22:32:16,807 - ERROR - Column gender not found in schema Some(StructType(StructField(TransactionID,StringType,true),StructField(CustomerID,StringType,true),StructField(CustomerDOB,StringType,true),StructField(CustGender,StringType,true),StructField(CustLocation,StringType,true),StructField(CustAccountBalance,StringType,true),StructField(TransactionDate,StringType,true),StructField(TransactionTime,StringType,true),StructField(TransactionAmount (INR),StringType,true))).
2025-03-21 22:36:10,692 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 22:36:11,950 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 22:36:12,949 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 22:36:13,077 - ERROR - ===== Failed to Transform Data for customers =====
2025-03-21 22:36:13,078 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `CustomerDOB` cannot be resolved. Did you mean one of the following? [`customer_id`, `gender`, `birth_date`, `location`, `TransactionID`].;
'Project [TransactionID#19016, customer_id#19034, birth_date#19045, gender#19055, location#19065, account_balance#19075, TransactionDate#19022, TransactionTime#19023, TransactionAmount (INR)#19024, CASE WHEN (cast(regexp_extract('CustomerDOB, (\d{1,2})/(\d{1,2})/(\d{2}), 3) as int) > 25) THEN to_date(concat(regexp_extract('CustomerDOB, (\d{1,2})/(\d{1,2})/, 1), /, regexp_extract('CustomerDOB, \d{1,2}/(\d{1,2})/, 1), /19, regexp_extract('CustomerDOB, \d{1,2}/\d{1,2}/(\d{2}), 1)), Some(d/M/yyyy), Some(Etc/UTC), false) ELSE to_date('CustomerDOB, Some(d/M/yy), Some(Etc/UTC), false) END AS CustomerDOB#19085]
+- Project [TransactionID#19016, customer_id#19034, birth_date#19045, gender#19055, location#19065, CustAccountBalance#19021 AS account_balance#19075, TransactionDate#19022, TransactionTime#19023, TransactionAmount (INR)#19024]
   +- Project [TransactionID#19016, customer_id#19034, birth_date#19045, gender#19055, CustLocation#19020 AS location#19065, CustAccountBalance#19021, TransactionDate#19022, TransactionTime#19023, TransactionAmount (INR)#19024]
      +- Project [TransactionID#19016, customer_id#19034, birth_date#19045, CustGender#19019 AS gender#19055, CustLocation#19020, CustAccountBalance#19021, TransactionDate#19022, TransactionTime#19023, TransactionAmount (INR)#19024]
         +- Project [TransactionID#19016, customer_id#19034, CustomerDOB#19018 AS birth_date#19045, CustGender#19019, CustLocation#19020, CustAccountBalance#19021, TransactionDate#19022, TransactionTime#19023, TransactionAmount (INR)#19024]
            +- Project [TransactionID#19016, CustomerID#19017 AS customer_id#19034, CustomerDOB#19018, CustGender#19019, CustLocation#19020, CustAccountBalance#19021, TransactionDate#19022, TransactionTime#19023, TransactionAmount (INR)#19024]
               +- Relation [TransactionID#19016,CustomerID#19017,CustomerDOB#19018,CustGender#19019,CustLocation#19020,CustAccountBalance#19021,TransactionDate#19022,TransactionTime#19023,TransactionAmount (INR)#19024] csv

2025-03-21 22:36:37,191 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 22:36:37,288 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 22:36:37,300 - INFO - ===== Finish Selecting Data process for table customers =====
2025-03-21 22:36:37,312 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 22:36:38,717 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:36:38,764 - INFO - ===== Truncated table customers successfully =====
2025-03-21 22:36:39,594 - ERROR - ===== Failed Load Data to the Database for table customers =====
2025-03-21 22:36:39,595 - ERROR - An error occurred while calling o6625.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 356.0 failed 1 times, most recent failure: Lost task 3.0 in stage 356.0 (TID 718) (pyspark executor driver): java.sql.BatchUpdateException: Batch entry 133 INSERT INTO customers ("customer_id","birth_date","gender","location","account_balance") VALUES ('C1013213','1800-01-01 +00','Unknown','KOLKATA',50939.87) was aborted: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C1013213, 1800-01-01, Unknown, KOLKATA, 50939.87, 2025-03-21 22:36:39.244146, 2025-03-21 22:36:39.244146).  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.postgresql.util.PSQLException: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C1013213, 1800-01-01, Unknown, KOLKATA, 50939.87, 2025-03-21 22:36:39.244146, 2025-03-21 22:36:39.244146).
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at jdk.internal.reflect.GeneratedMethodAccessor310.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.BatchUpdateException: Batch entry 133 INSERT INTO customers ("customer_id","birth_date","gender","location","account_balance") VALUES ('C1013213','1800-01-01 +00','Unknown','KOLKATA',50939.87) was aborted: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C1013213, 1800-01-01, Unknown, KOLKATA, 50939.87, 2025-03-21 22:36:39.244146, 2025-03-21 22:36:39.244146).  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.postgresql.util.PSQLException: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C1013213, 1800-01-01, Unknown, KOLKATA, 50939.87, 2025-03-21 22:36:39.244146, 2025-03-21 22:36:39.244146).
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

2025-03-21 22:38:20,541 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:38:20,623 - INFO - ===== Truncated table customers successfully =====
2025-03-21 22:38:21,150 - ERROR - ===== Failed Load Data to the Database for table customers =====
2025-03-21 22:38:21,151 - ERROR - An error occurred while calling o6631.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 357.0 failed 1 times, most recent failure: Lost task 3.0 in stage 357.0 (TID 727) (pyspark executor driver): java.sql.BatchUpdateException: Batch entry 133 INSERT INTO customers ("customer_id","birth_date","gender","location","account_balance") VALUES ('C1013213','1800-01-01 +00','Unknown','KOLKATA',50939.87) was aborted: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C1013213, 1800-01-01, Unknown, KOLKATA, 50939.87, 2025-03-21 22:38:20.935087, 2025-03-21 22:38:20.935087).  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.postgresql.util.PSQLException: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C1013213, 1800-01-01, Unknown, KOLKATA, 50939.87, 2025-03-21 22:38:20.935087, 2025-03-21 22:38:20.935087).
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at jdk.internal.reflect.GeneratedMethodAccessor310.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.BatchUpdateException: Batch entry 133 INSERT INTO customers ("customer_id","birth_date","gender","location","account_balance") VALUES ('C1013213','1800-01-01 +00','Unknown','KOLKATA',50939.87) was aborted: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C1013213, 1800-01-01, Unknown, KOLKATA, 50939.87, 2025-03-21 22:38:20.935087, 2025-03-21 22:38:20.935087).  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.postgresql.util.PSQLException: ERROR: new row for relation "customers" violates check constraint "customers_gender_check"
  Detail: Failing row contains (C1013213, 1800-01-01, Unknown, KOLKATA, 50939.87, 2025-03-21 22:38:20.935087, 2025-03-21 22:38:20.935087).
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

2025-03-21 22:38:34,778 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 22:38:36,145 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 22:38:37,792 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 22:38:37,857 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 22:38:37,874 - INFO - ===== Finish Selecting Data process for table customers =====
2025-03-21 22:38:37,892 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 22:38:39,618 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:38:39,701 - INFO - ===== Truncated table customers successfully =====
2025-03-21 22:38:53,917 - INFO - ===== Finished Load Data to the Database =====
2025-03-21 22:39:28,822 - INFO - ===== Start Banking Data Pipeline =====
2025-03-21 22:39:28,832 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 22:39:30,102 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 22:39:30,104 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 22:39:31,267 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 22:39:31,270 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-21 22:39:31,366 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-21 22:39:31,369 - INFO - ===== Start Extracting education_status data =====
2025-03-21 22:39:31,395 - INFO - ===== Finish Extracting education_status data =====
2025-03-21 22:39:31,398 - INFO - ===== Start Extracting marital_status data =====
2025-03-21 22:39:31,420 - INFO - ===== Finish Extracting marital_status data =====
2025-03-21 22:39:31,422 - INFO - ===== Start Transforming Data for transactions =====
2025-03-21 22:39:31,485 - INFO - ===== Start Selecting Data process for table transactions =====
2025-03-21 22:39:31,500 - INFO - ===== Finish Selecting Data process for table transactions =====
2025-03-21 22:39:31,501 - INFO - ===== Finished Transforming Data for transactions =====
2025-03-21 22:39:31,502 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 22:39:31,549 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 22:39:31,561 - INFO - ===== Finish Selecting Data process for table customers =====
2025-03-21 22:39:31,577 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 22:39:31,578 - INFO - ===== Start Transforming Data for marketing_campaign_deposit =====
2025-03-21 22:39:31,604 - INFO - ===== Start Selecting Data process for table marketing_campaign_deposit =====
2025-03-21 22:39:31,649 - ERROR - ===== Failed Selecting Data process for table marketing_campaign_deposit =====
2025-03-21 22:39:31,650 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `days_since_last_campaign` cannot be resolved. Did you mean one of the following? [`campaign`, `loan_data_id`, `created_at`, `duration_in_year`, `education_id`].;
'Project [loan_data_id#19424, age#19425, job#19426, marital_id#19427, education_id#19428, default#19429, balance#19675, housing#19431, loan#19432, contact#19433, day#19434, month#19435, duration#19436, duration_in_year#19696, campaign#19437, 'days_since_last_campaign, 'previous_campaign_contacts, 'previous_campaign_outcome, subscribed_deposit#19441]
+- Project [loan_data_id#19424, age#19425, job#19426, marital_id#19427, education_id#19428, default#19429, balance#19675, housing#19431, loan#19432, contact#19433, day#19434, month#19435, duration#19436, campaign#19437, pdays#19438, previous#19439, poutcome#19440, subscribed_deposit#19441, created_at#19442, updated_at#19443, cast(round((cast(duration#19436 as double) / cast(365 as double)), 0) as int) AS duration_in_year#19696]
   +- Project [loan_data_id#19424, age#19425, job#19426, marital_id#19427, education_id#19428, default#19429, cast(regexp_replace(balance#19430, \$, , 1) as int) AS balance#19675, housing#19431, loan#19432, contact#19433, day#19434, month#19435, duration#19436, campaign#19437, pdays#19438, previous#19439, poutcome#19440, subscribed_deposit#19441, created_at#19442, updated_at#19443]
      +- Relation [loan_data_id#19424,age#19425,job#19426,marital_id#19427,education_id#19428,default#19429,balance#19430,housing#19431,loan#19432,contact#19433,day#19434,month#19435,duration#19436,campaign#19437,pdays#19438,previous#19439,poutcome#19440,subscribed_deposit#19441,created_at#19442,updated_at#19443] JDBCRelation(marketing_campaign_deposit) [numPartitions=1]

2025-03-21 22:39:31,654 - ERROR - ===== Failed to Transform Data for marketing_campaign_deposit =====
2025-03-21 22:39:31,654 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `days_since_last_campaign` cannot be resolved. Did you mean one of the following? [`campaign`, `loan_data_id`, `created_at`, `duration_in_year`, `education_id`].;
'Project [loan_data_id#19424, age#19425, job#19426, marital_id#19427, education_id#19428, default#19429, balance#19675, housing#19431, loan#19432, contact#19433, day#19434, month#19435, duration#19436, duration_in_year#19696, campaign#19437, 'days_since_last_campaign, 'previous_campaign_contacts, 'previous_campaign_outcome, subscribed_deposit#19441]
+- Project [loan_data_id#19424, age#19425, job#19426, marital_id#19427, education_id#19428, default#19429, balance#19675, housing#19431, loan#19432, contact#19433, day#19434, month#19435, duration#19436, campaign#19437, pdays#19438, previous#19439, poutcome#19440, subscribed_deposit#19441, created_at#19442, updated_at#19443, cast(round((cast(duration#19436 as double) / cast(365 as double)), 0) as int) AS duration_in_year#19696]
   +- Project [loan_data_id#19424, age#19425, job#19426, marital_id#19427, education_id#19428, default#19429, cast(regexp_replace(balance#19430, \$, , 1) as int) AS balance#19675, housing#19431, loan#19432, contact#19433, day#19434, month#19435, duration#19436, campaign#19437, pdays#19438, previous#19439, poutcome#19440, subscribed_deposit#19441, created_at#19442, updated_at#19443]
      +- Relation [loan_data_id#19424,age#19425,job#19426,marital_id#19427,education_id#19428,default#19429,balance#19430,housing#19431,loan#19432,contact#19433,day#19434,month#19435,duration#19436,campaign#19437,pdays#19438,previous#19439,poutcome#19440,subscribed_deposit#19441,created_at#19442,updated_at#19443] JDBCRelation(marketing_campaign_deposit) [numPartitions=1]

2025-03-21 22:39:31,658 - ERROR - ===== Data Pipeline Failed =====
2025-03-21 22:39:31,659 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `days_since_last_campaign` cannot be resolved. Did you mean one of the following? [`campaign`, `loan_data_id`, `created_at`, `duration_in_year`, `education_id`].;
'Project [loan_data_id#19424, age#19425, job#19426, marital_id#19427, education_id#19428, default#19429, balance#19675, housing#19431, loan#19432, contact#19433, day#19434, month#19435, duration#19436, duration_in_year#19696, campaign#19437, 'days_since_last_campaign, 'previous_campaign_contacts, 'previous_campaign_outcome, subscribed_deposit#19441]
+- Project [loan_data_id#19424, age#19425, job#19426, marital_id#19427, education_id#19428, default#19429, balance#19675, housing#19431, loan#19432, contact#19433, day#19434, month#19435, duration#19436, campaign#19437, pdays#19438, previous#19439, poutcome#19440, subscribed_deposit#19441, created_at#19442, updated_at#19443, cast(round((cast(duration#19436 as double) / cast(365 as double)), 0) as int) AS duration_in_year#19696]
   +- Project [loan_data_id#19424, age#19425, job#19426, marital_id#19427, education_id#19428, default#19429, cast(regexp_replace(balance#19430, \$, , 1) as int) AS balance#19675, housing#19431, loan#19432, contact#19433, day#19434, month#19435, duration#19436, campaign#19437, pdays#19438, previous#19439, poutcome#19440, subscribed_deposit#19441, created_at#19442, updated_at#19443]
      +- Relation [loan_data_id#19424,age#19425,job#19426,marital_id#19427,education_id#19428,default#19429,balance#19430,housing#19431,loan#19432,contact#19433,day#19434,month#19435,duration#19436,campaign#19437,pdays#19438,previous#19439,poutcome#19440,subscribed_deposit#19441,created_at#19442,updated_at#19443] JDBCRelation(marketing_campaign_deposit) [numPartitions=1]

2025-03-21 22:41:44,548 - INFO - ===== Start Banking Data Pipeline =====
2025-03-21 22:41:44,556 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 22:41:46,066 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 22:41:46,070 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 22:41:47,341 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 22:41:47,346 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-21 22:41:47,375 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-21 22:41:47,380 - INFO - ===== Start Extracting education_status data =====
2025-03-21 22:41:47,402 - INFO - ===== Finish Extracting education_status data =====
2025-03-21 22:41:47,406 - INFO - ===== Start Extracting marital_status data =====
2025-03-21 22:41:47,429 - INFO - ===== Finish Extracting marital_status data =====
2025-03-21 22:41:47,431 - INFO - ===== Start Transforming Data for transactions =====
2025-03-21 22:41:47,485 - INFO - ===== Start Selecting Data process for table transactions =====
2025-03-21 22:41:47,505 - INFO - ===== Finish Selecting Data process for table transactions =====
2025-03-21 22:41:47,505 - INFO - ===== Finished Transforming Data for transactions =====
2025-03-21 22:41:47,506 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 22:41:47,563 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 22:41:47,579 - INFO - ===== Finish Selecting Data process for table customers =====
2025-03-21 22:41:47,598 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 22:41:47,600 - INFO - ===== Start Transforming Data for marketing_campaign_deposit =====
2025-03-21 22:41:47,650 - INFO - ===== Start Selecting Data process for table marketing_campaign_deposit =====
2025-03-21 22:41:47,697 - INFO - ===== Finish Selecting Data process for table marketing_campaign_deposit =====
2025-03-21 22:41:47,698 - INFO - ===== Finished Transforming Data for marketing_campaign_deposit =====
2025-03-21 22:41:47,699 - INFO - ===== Start Transforming Data for education_status =====
2025-03-21 22:41:47,700 - INFO - ===== Start Selecting Data process for table education_status =====
2025-03-21 22:41:47,707 - INFO - ===== Finish Selecting Data process for table education_status =====
2025-03-21 22:41:47,708 - INFO - ===== Finished Transforming Data for education_status =====
2025-03-21 22:41:47,709 - INFO - ===== Start Transforming Data for marital_status =====
2025-03-21 22:41:47,710 - INFO - ===== Start Selecting Data process for table marital_status =====
2025-03-21 22:41:47,718 - INFO - ===== Finish Selecting Data process for table marital_status =====
2025-03-21 22:41:47,719 - INFO - ===== Finished Transforming Data for marital_status =====
2025-03-21 22:41:47,720 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:41:47,738 - INFO - ===== Truncated table transactions successfully =====
2025-03-21 22:42:04,766 - INFO - ===== Finished Load Data to the Database =====
2025-03-21 22:42:04,767 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:42:04,836 - INFO - ===== Truncated table customers successfully =====
2025-03-21 22:42:17,461 - INFO - ===== Finished Load Data to the Database =====
2025-03-21 22:42:17,463 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:42:17,483 - INFO - ===== Truncated table marketing_campaign_deposit successfully =====
2025-03-21 22:42:18,111 - ERROR - ===== Failed Load Data to the Database for table marketing_campaign_deposit =====
2025-03-21 22:42:18,112 - ERROR - An error occurred while calling o7018.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 366.0 failed 1 times, most recent failure: Lost task 0.0 in stage 366.0 (TID 765) (pyspark executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO marketing_campaign_deposit ("loan_data_id","age","job","marital_id","education_id","default","balance","housing","loan","contact","day","month","duration","duration_in_year","campaign","days_since_last_campaign","previous_campaign_contacts","previous_campaign_outcome","subscribed_deposit") VALUES (1,58,'management',1,1,'FALSE',2143,'TRUE','FALSE','unknown',5,'may',261,1,1,-1,0,'unknown','FALSE') was aborted: ERROR: insert or update on table "marketing_campaign_deposit" violates foreign key constraint "marketing_campaign_deposit_marital_id_fkey"
  Detail: Key (marital_id)=(1) is not present in table "marital_status".  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.postgresql.util.PSQLException: ERROR: insert or update on table "marketing_campaign_deposit" violates foreign key constraint "marketing_campaign_deposit_marital_id_fkey"
  Detail: Key (marital_id)=(1) is not present in table "marital_status".
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at jdk.internal.reflect.GeneratedMethodAccessor310.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO marketing_campaign_deposit ("loan_data_id","age","job","marital_id","education_id","default","balance","housing","loan","contact","day","month","duration","duration_in_year","campaign","days_since_last_campaign","previous_campaign_contacts","previous_campaign_outcome","subscribed_deposit") VALUES (1,58,'management',1,1,'FALSE',2143,'TRUE','FALSE','unknown',5,'may',261,1,1,-1,0,'unknown','FALSE') was aborted: ERROR: insert or update on table "marketing_campaign_deposit" violates foreign key constraint "marketing_campaign_deposit_marital_id_fkey"
  Detail: Key (marital_id)=(1) is not present in table "marital_status".  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.postgresql.util.PSQLException: ERROR: insert or update on table "marketing_campaign_deposit" violates foreign key constraint "marketing_campaign_deposit_marital_id_fkey"
  Detail: Key (marital_id)=(1) is not present in table "marital_status".
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

2025-03-21 22:42:18,164 - ERROR - ===== Data Pipeline Failed =====
2025-03-21 22:42:18,165 - ERROR - An error occurred while calling o7018.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 366.0 failed 1 times, most recent failure: Lost task 0.0 in stage 366.0 (TID 765) (pyspark executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO marketing_campaign_deposit ("loan_data_id","age","job","marital_id","education_id","default","balance","housing","loan","contact","day","month","duration","duration_in_year","campaign","days_since_last_campaign","previous_campaign_contacts","previous_campaign_outcome","subscribed_deposit") VALUES (1,58,'management',1,1,'FALSE',2143,'TRUE','FALSE','unknown',5,'may',261,1,1,-1,0,'unknown','FALSE') was aborted: ERROR: insert or update on table "marketing_campaign_deposit" violates foreign key constraint "marketing_campaign_deposit_marital_id_fkey"
  Detail: Key (marital_id)=(1) is not present in table "marital_status".  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.postgresql.util.PSQLException: ERROR: insert or update on table "marketing_campaign_deposit" violates foreign key constraint "marketing_campaign_deposit_marital_id_fkey"
  Detail: Key (marital_id)=(1) is not present in table "marital_status".
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at jdk.internal.reflect.GeneratedMethodAccessor310.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO marketing_campaign_deposit ("loan_data_id","age","job","marital_id","education_id","default","balance","housing","loan","contact","day","month","duration","duration_in_year","campaign","days_since_last_campaign","previous_campaign_contacts","previous_campaign_outcome","subscribed_deposit") VALUES (1,58,'management',1,1,'FALSE',2143,'TRUE','FALSE','unknown',5,'may',261,1,1,-1,0,'unknown','FALSE') was aborted: ERROR: insert or update on table "marketing_campaign_deposit" violates foreign key constraint "marketing_campaign_deposit_marital_id_fkey"
  Detail: Key (marital_id)=(1) is not present in table "marital_status".  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2402)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2134)
	at org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1491)
	at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1516)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: org.postgresql.util.PSQLException: ERROR: insert or update on table "marketing_campaign_deposit" violates foreign key constraint "marketing_campaign_deposit_marital_id_fkey"
  Detail: Key (marital_id)=(1) is not present in table "marital_status".
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)
	... 24 more

2025-03-21 22:44:30,074 - INFO - ===== Start Banking Data Pipeline =====
2025-03-21 22:44:30,081 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 22:44:31,156 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 22:44:31,161 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-21 22:44:32,482 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-21 22:44:32,485 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-21 22:44:32,511 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-21 22:44:32,515 - INFO - ===== Start Extracting education_status data =====
2025-03-21 22:44:32,536 - INFO - ===== Finish Extracting education_status data =====
2025-03-21 22:44:32,540 - INFO - ===== Start Extracting marital_status data =====
2025-03-21 22:44:32,559 - INFO - ===== Finish Extracting marital_status data =====
2025-03-21 22:44:32,560 - INFO - ===== Start Transforming Data for transactions =====
2025-03-21 22:44:32,613 - INFO - ===== Start Selecting Data process for table transactions =====
2025-03-21 22:44:32,626 - INFO - ===== Finish Selecting Data process for table transactions =====
2025-03-21 22:44:32,627 - INFO - ===== Finished Transforming Data for transactions =====
2025-03-21 22:44:32,627 - INFO - ===== Start Transforming Data for customers =====
2025-03-21 22:44:32,664 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-21 22:44:32,676 - INFO - ===== Finish Selecting Data process for table customers =====
2025-03-21 22:44:32,688 - INFO - ===== Finished Transforming Data for customers =====
2025-03-21 22:44:32,689 - INFO - ===== Start Transforming Data for marketing_campaign_deposit =====
2025-03-21 22:44:32,715 - INFO - ===== Start Selecting Data process for table marketing_campaign_deposit =====
2025-03-21 22:44:32,757 - INFO - ===== Finish Selecting Data process for table marketing_campaign_deposit =====
2025-03-21 22:44:32,758 - INFO - ===== Finished Transforming Data for marketing_campaign_deposit =====
2025-03-21 22:44:32,759 - INFO - ===== Start Transforming Data for education_status =====
2025-03-21 22:44:32,760 - INFO - ===== Start Selecting Data process for table education_status =====
2025-03-21 22:44:32,767 - INFO - ===== Finish Selecting Data process for table education_status =====
2025-03-21 22:44:32,768 - INFO - ===== Finished Transforming Data for education_status =====
2025-03-21 22:44:32,769 - INFO - ===== Start Transforming Data for marital_status =====
2025-03-21 22:44:32,770 - INFO - ===== Start Selecting Data process for table marital_status =====
2025-03-21 22:44:32,779 - INFO - ===== Finish Selecting Data process for table marital_status =====
2025-03-21 22:44:32,780 - INFO - ===== Finished Transforming Data for marital_status =====
2025-03-21 22:44:32,781 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:44:32,799 - INFO - ===== Truncated table education_status successfully =====
2025-03-21 22:44:32,930 - INFO - ===== Finished Load Data to the Database =====
2025-03-21 22:44:32,931 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:44:32,946 - INFO - ===== Truncated table marital_status successfully =====
2025-03-21 22:44:33,052 - INFO - ===== Finished Load Data to the Database =====
2025-03-21 22:44:33,053 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:44:33,097 - INFO - ===== Truncated table customers successfully =====
2025-03-21 22:44:42,845 - INFO - ===== Finished Load Data to the Database =====
2025-03-21 22:44:42,847 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:44:42,865 - INFO - ===== Truncated table transactions successfully =====
2025-03-21 22:44:57,688 - INFO - ===== Finished Load Data to the Database =====
2025-03-21 22:44:57,689 - INFO - ===== Start Load Data to the Database =====
2025-03-21 22:44:57,707 - INFO - ===== Truncated table marketing_campaign_deposit successfully =====
2025-03-21 22:44:59,216 - INFO - ===== Finished Load Data to the Database =====
2025-03-21 22:44:59,217 - INFO - ===== Finish Banking Data Pipeline =====
2025-03-22 02:21:24,832 - ERROR - No such comm target registered: jupyter.widget.control
2025-03-22 02:21:24,845 - WARNING - No such comm: c135715f-1520-44d5-a583-04d5f2e94d04
2025-03-22 02:32:24,235 - INFO - ===== Start Banking Data Pipeline =====
2025-03-22 02:32:24,244 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-22 02:32:25,456 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-22 02:32:25,459 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-22 02:32:26,680 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-22 02:32:26,684 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-22 02:32:26,794 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-22 02:32:26,797 - INFO - ===== Start Extracting education_status data =====
2025-03-22 02:32:26,819 - INFO - ===== Finish Extracting education_status data =====
2025-03-22 02:32:26,823 - INFO - ===== Start Extracting marital_status data =====
2025-03-22 02:32:26,842 - INFO - ===== Finish Extracting marital_status data =====
2025-03-22 02:32:26,843 - INFO - ===== Start Transforming Data for transactions =====
2025-03-22 02:32:26,909 - INFO - ===== Start Selecting Data process for table transactions =====
2025-03-22 02:32:26,924 - INFO - ===== Finish Selecting Data process for table transactions =====
2025-03-22 02:32:26,925 - INFO - ===== Finished Transforming Data for transactions =====
2025-03-22 02:32:26,926 - INFO - ===== Start Transforming Data for customers =====
2025-03-22 02:32:26,971 - INFO - ===== Start Selecting Data process for table customers =====
2025-03-22 02:32:26,987 - INFO - ===== Finish Selecting Data process for table customers =====
2025-03-22 02:32:27,005 - INFO - ===== Finished Transforming Data for customers =====
2025-03-22 02:32:27,006 - INFO - ===== Start Transforming Data for marketing_campaign_deposit =====
2025-03-22 02:32:27,031 - INFO - ===== Start Selecting Data process for table marketing_campaign_deposit =====
2025-03-22 02:32:27,070 - INFO - ===== Finish Selecting Data process for table marketing_campaign_deposit =====
2025-03-22 02:32:27,071 - INFO - ===== Finished Transforming Data for marketing_campaign_deposit =====
2025-03-22 02:32:27,072 - INFO - ===== Start Transforming Data for education_status =====
2025-03-22 02:32:27,073 - INFO - ===== Start Selecting Data process for table education_status =====
2025-03-22 02:32:27,080 - INFO - ===== Finish Selecting Data process for table education_status =====
2025-03-22 02:32:27,081 - INFO - ===== Finished Transforming Data for education_status =====
2025-03-22 02:32:27,081 - INFO - ===== Start Transforming Data for marital_status =====
2025-03-22 02:32:27,082 - INFO - ===== Start Selecting Data process for table marital_status =====
2025-03-22 02:32:27,090 - INFO - ===== Finish Selecting Data process for table marital_status =====
2025-03-22 02:32:27,091 - INFO - ===== Finished Transforming Data for marital_status =====
2025-03-22 02:32:27,092 - INFO - ===== Start Load Data to the Database =====
2025-03-22 02:32:27,208 - INFO - ===== Truncated table education_status successfully =====
2025-03-22 02:32:27,371 - INFO - ===== Finished Load Data to the Database =====
2025-03-22 02:32:27,372 - INFO - ===== Start Load Data to the Database =====
2025-03-22 02:32:27,392 - INFO - ===== Truncated table marital_status successfully =====
2025-03-22 02:32:27,538 - INFO - ===== Finished Load Data to the Database =====
2025-03-22 02:32:27,539 - INFO - ===== Start Load Data to the Database =====
2025-03-22 02:32:27,767 - INFO - ===== Truncated table customers successfully =====
2025-03-22 02:32:45,457 - INFO - ===== Finished Load Data to the Database =====
2025-03-22 02:32:45,459 - INFO - ===== Start Load Data to the Database =====
2025-03-22 02:32:45,481 - INFO - ===== Truncated table transactions successfully =====
2025-03-22 02:33:08,436 - INFO - ===== Finished Load Data to the Database =====
2025-03-22 02:33:08,437 - INFO - ===== Start Load Data to the Database =====
2025-03-22 02:33:08,482 - INFO - ===== Truncated table marketing_campaign_deposit successfully =====
2025-03-22 02:33:10,561 - INFO - ===== Finished Load Data to the Database =====
2025-03-22 02:33:10,562 - INFO - ===== Finish Banking Data Pipeline =====
2025-03-22 02:47:10,460 - INFO - ===== Start Banking Data Pipeline =====
2025-03-22 02:47:17,814 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-22 02:47:23,087 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-22 02:47:23,094 - INFO - ===== Start Extracting new_bank_transaction data =====
2025-03-22 02:47:24,388 - INFO - ===== Finish Extracting new_bank_transaction data =====
2025-03-22 02:47:24,394 - INFO - ===== Start Extracting marketing_campaign_deposit data =====
2025-03-22 02:47:24,811 - INFO - ===== Finish Extracting marketing_campaign_deposit data =====
2025-03-22 02:47:24,818 - INFO - ===== Start Extracting education_status data =====
2025-03-22 02:47:24,872 - INFO - ===== Finish Extracting education_status data =====
2025-03-22 02:47:24,877 - INFO - ===== Start Extracting marital_status data =====
2025-03-22 02:47:24,929 - INFO - ===== Finish Extracting marital_status data =====
2025-03-22 02:47:24,930 - INFO - ===== Start Transforming Data for transactions =====
2025-03-22 02:47:25,270 - ERROR - ===== Failed to Transform Data for transactions =====
2025-03-22 02:47:25,271 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `TransactionDate` cannot be resolved. Did you mean one of the following? [`transaction_date`, `transaction_id`, `transaction_time`, `transaction_amount`, `CustGender`].;
'Project [transaction_id#126, customer_id#137, CustomerDOB#19, CustGender#20, CustLocation#21, CustAccountBalance#22, to_date('TransactionDate, Some(d/M/yy), Some(Etc/UTC), false) AS transaction_date#177, transaction_time#157, transaction_amount#167]
+- Project [transaction_id#126, customer_id#137, CustomerDOB#19, CustGender#20, CustLocation#21, CustAccountBalance#22, transaction_date#147, transaction_time#157, TransactionAmount (INR)#25 AS transaction_amount#167]
   +- Project [transaction_id#126, customer_id#137, CustomerDOB#19, CustGender#20, CustLocation#21, CustAccountBalance#22, transaction_date#147, TransactionTime#24 AS transaction_time#157, TransactionAmount (INR)#25]
      +- Project [transaction_id#126, customer_id#137, CustomerDOB#19, CustGender#20, CustLocation#21, CustAccountBalance#22, TransactionDate#23 AS transaction_date#147, TransactionTime#24, TransactionAmount (INR)#25]
         +- Project [transaction_id#126, CustomerID#18 AS customer_id#137, CustomerDOB#19, CustGender#20, CustLocation#21, CustAccountBalance#22, TransactionDate#23, TransactionTime#24, TransactionAmount (INR)#25]
            +- Project [TransactionID#17 AS transaction_id#126, CustomerID#18, CustomerDOB#19, CustGender#20, CustLocation#21, CustAccountBalance#22, TransactionDate#23, TransactionTime#24, TransactionAmount (INR)#25]
               +- Relation [TransactionID#17,CustomerID#18,CustomerDOB#19,CustGender#20,CustLocation#21,CustAccountBalance#22,TransactionDate#23,TransactionTime#24,TransactionAmount (INR)#25] csv

2025-03-22 02:47:25,278 - ERROR - ===== Data Pipeline Failed =====
2025-03-22 02:47:25,278 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `TransactionDate` cannot be resolved. Did you mean one of the following? [`transaction_date`, `transaction_id`, `transaction_time`, `transaction_amount`, `CustGender`].;
'Project [transaction_id#126, customer_id#137, CustomerDOB#19, CustGender#20, CustLocation#21, CustAccountBalance#22, to_date('TransactionDate, Some(d/M/yy), Some(Etc/UTC), false) AS transaction_date#177, transaction_time#157, transaction_amount#167]
+- Project [transaction_id#126, customer_id#137, CustomerDOB#19, CustGender#20, CustLocation#21, CustAccountBalance#22, transaction_date#147, transaction_time#157, TransactionAmount (INR)#25 AS transaction_amount#167]
   +- Project [transaction_id#126, customer_id#137, CustomerDOB#19, CustGender#20, CustLocation#21, CustAccountBalance#22, transaction_date#147, TransactionTime#24 AS transaction_time#157, TransactionAmount (INR)#25]
      +- Project [transaction_id#126, customer_id#137, CustomerDOB#19, CustGender#20, CustLocation#21, CustAccountBalance#22, TransactionDate#23 AS transaction_date#147, TransactionTime#24, TransactionAmount (INR)#25]
         +- Project [transaction_id#126, CustomerID#18 AS customer_id#137, CustomerDOB#19, CustGender#20, CustLocation#21, CustAccountBalance#22, TransactionDate#23, TransactionTime#24, TransactionAmount (INR)#25]
            +- Project [TransactionID#17 AS transaction_id#126, CustomerID#18, CustomerDOB#19, CustGender#20, CustLocation#21, CustAccountBalance#22, TransactionDate#23, TransactionTime#24, TransactionAmount (INR)#25]
               +- Relation [TransactionID#17,CustomerID#18,CustomerDOB#19,CustGender#20,CustLocation#21,CustAccountBalance#22,TransactionDate#23,TransactionTime#24,TransactionAmount (INR)#25] csv

2025-03-22 02:47:25,308 - INFO - Closing down clientserver connection
