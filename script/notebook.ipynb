{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "89faad26-a805-4ba6-a0dc-1c0058361777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5970c651-b5dd-4de0-95e3-963d0ca536a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Exercise Data Pipeline with PySpark Week 6\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5489b9bc-664a-4af8-8bde-62015f07a610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pyspark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Exercise Data Pipeline with PySpark Week 6</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1d852d4690>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "51a34e90-8cd4-4c96-a709-1df5f5d1778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " TransactionID           | T642232     \n",
      " CustomerID              | C1010028    \n",
      " CustomerDOB             | 25/8/88     \n",
      " CustGender              | F           \n",
      " CustLocation            | DELHI       \n",
      " CustAccountBalance      | 296828.37   \n",
      " TransactionDate         | 29/8/16     \n",
      " TransactionTime         | 95212       \n",
      " TransactionAmount (INR) | 557         \n",
      "-RECORD 1------------------------------\n",
      " TransactionID           | T87414      \n",
      " CustomerID              | C1010035    \n",
      " CustomerDOB             | 2/3/92      \n",
      " CustGender              | M           \n",
      " CustLocation            | MUMBAI      \n",
      " CustAccountBalance      | 7284.42     \n",
      " TransactionDate         | 1/8/16      \n",
      " TransactionTime         | 111917      \n",
      " TransactionAmount (INR) | 50          \n",
      "-RECORD 2------------------------------\n",
      " TransactionID           | T560676     \n",
      " CustomerID              | C1010035_2  \n",
      " CustomerDOB             | 9/6/80      \n",
      " CustGender              | M           \n",
      " CustLocation            | NAVI MUMBAI \n",
      " CustAccountBalance      | 378013.09   \n",
      " TransactionDate         | 27/8/16     \n",
      " TransactionTime         | 185011      \n",
      " TransactionAmount (INR) | 700         \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transaction = spark.read.csv(\"data/new_bank_transaction.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "\n",
    "df_transaction.show(3, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311f74c-c28e-42b3-9bac-9f0f32341a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"data/new_bank_transaction.csv/\"\n",
    "\n",
    "df_ratings = spark.read.csv(directory + \"part-*.csv\", header=True)\n",
    "\n",
    "df_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa47860c-d92c-4dca-8e25-5a1126d88cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init vars\n",
    "DB_URL = \"jdbc:postgresql://source_db:5432/source\"\n",
    "DB_TABLE = \"marketing_campaign_deposit\" \n",
    "DB_USER = \"postgres\"\n",
    "DB_PASS = \"postgres\"\n",
    "\n",
    "# set config\n",
    "jdbc_url = DB_URL\n",
    "table_name = DB_TABLE\n",
    "connection_properties = {\n",
    "    \"user\": DB_USER,\n",
    "    \"password\": DB_PASS,\n",
    "    \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9baeafa4-4ebe-4e72-ad05-0c42f8d783b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marketing = spark \\\n",
    "              .read \\\n",
    "              .jdbc(url = jdbc_url,\n",
    "                    table = table_name,\n",
    "                    properties = connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdf48b53-4a0d-469a-b4c3-1a461a996a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------\n",
      " loan_data_id       | 1                          \n",
      " age                | 58                         \n",
      " job                | management                 \n",
      " marital_id         | 1                          \n",
      " education_id       | 1                          \n",
      " default            | false                      \n",
      " balance            | $2143                      \n",
      " housing            | true                       \n",
      " loan               | false                      \n",
      " contact            | unknown                    \n",
      " day                | 5                          \n",
      " month              | may                        \n",
      " duration           | 261                        \n",
      " campaign           | 1                          \n",
      " pdays              | -1                         \n",
      " previous           | 0                          \n",
      " poutcome           | unknown                    \n",
      " subscribed_deposit | false                      \n",
      " created_at         | 2025-02-28 15:59:11.102813 \n",
      " updated_at         | 2025-02-28 15:59:11.102813 \n",
      "-RECORD 1----------------------------------------\n",
      " loan_data_id       | 2                          \n",
      " age                | 44                         \n",
      " job                | technician                 \n",
      " marital_id         | 2                          \n",
      " education_id       | 2                          \n",
      " default            | false                      \n",
      " balance            | $29                        \n",
      " housing            | true                       \n",
      " loan               | false                      \n",
      " contact            | unknown                    \n",
      " day                | 5                          \n",
      " month              | may                        \n",
      " duration           | 151                        \n",
      " campaign           | 1                          \n",
      " pdays              | -1                         \n",
      " previous           | 0                          \n",
      " poutcome           | unknown                    \n",
      " subscribed_deposit | false                      \n",
      " created_at         | 2025-02-28 15:59:11.102813 \n",
      " updated_at         | 2025-02-28 15:59:11.102813 \n",
      "-RECORD 2----------------------------------------\n",
      " loan_data_id       | 3                          \n",
      " age                | 33                         \n",
      " job                | entrepreneur               \n",
      " marital_id         | 1                          \n",
      " education_id       | 2                          \n",
      " default            | false                      \n",
      " balance            | $2                         \n",
      " housing            | true                       \n",
      " loan               | true                       \n",
      " contact            | unknown                    \n",
      " day                | 5                          \n",
      " month              | may                        \n",
      " duration           | 76                         \n",
      " campaign           | 1                          \n",
      " pdays              | -1                         \n",
      " previous           | 0                          \n",
      " poutcome           | unknown                    \n",
      " subscribed_deposit | false                      \n",
      " created_at         | 2025-02-28 15:59:11.102813 \n",
      " updated_at         | 2025-02-28 15:59:11.102813 \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_marketing.show(3, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "355d472f-cb3a-45c3-99d1-6a07969be19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_education = spark.read.jdbc(url=jdbc_url, table=\"education_status\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b85a7c5-2fb8-47e9-aa97-eb839650da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- education_id: integer (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_education.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34fa82e9-4bce-458b-bb9f-3d5884741762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " education_id | 1                          \n",
      " value        | tertiary                   \n",
      " created_at   | 2025-02-28 15:31:04.358235 \n",
      " updated_at   | 2025-02-28 15:31:04.358235 \n",
      "-RECORD 1----------------------------------\n",
      " education_id | 2                          \n",
      " value        | secondary                  \n",
      " created_at   | 2025-02-28 15:31:04.358235 \n",
      " updated_at   | 2025-02-28 15:31:04.358235 \n",
      "-RECORD 2----------------------------------\n",
      " education_id | 3                          \n",
      " value        | unknown                    \n",
      " created_at   | 2025-02-28 15:31:04.358235 \n",
      " updated_at   | 2025-02-28 15:31:04.358235 \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_education.show(3, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b7b5387-0d8c-47fd-850f-f4a1927aeaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marital = spark.read.jdbc(url=jdbc_url, table=\"marital_status\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2030f6f8-2764-4839-af6a-649f1451dc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marital_id: integer (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_marital.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c13c79a3-a375-4ed9-87bd-8559be70ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " marital_id | 1                          \n",
      " value      | married                    \n",
      " created_at | 2025-02-28 15:31:01.502136 \n",
      " updated_at | 2025-02-28 15:31:01.502136 \n",
      "-RECORD 1--------------------------------\n",
      " marital_id | 2                          \n",
      " value      | single                     \n",
      " created_at | 2025-02-28 15:31:01.502136 \n",
      " updated_at | 2025-02-28 15:31:01.502136 \n",
      "-RECORD 2--------------------------------\n",
      " marital_id | 3                          \n",
      " value      | divorced                   \n",
      " created_at | 2025-02-28 15:31:01.502136 \n",
      " updated_at | 2025-02-28 15:31:01.502136 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_marital.show(3, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb4ebe-da24-43d1-93e8-4767f0030df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "07abbb1a-c641-4d71-8d54-53d014f71d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " loan_data_id               | 0   \n",
      " age                        | 0   \n",
      " job                        | 0   \n",
      " marital_id                 | 0   \n",
      " education_id               | 0   \n",
      " default                    | 0   \n",
      " balance                    | 0   \n",
      " housing                    | 0   \n",
      " loan                       | 0   \n",
      " contact                    | 0   \n",
      " day                        | 0   \n",
      " month                      | 0   \n",
      " duration                   | 0   \n",
      " campaign                   | 0   \n",
      " days_since_last_campaign   | 0   \n",
      " previous_campaign_contacts | 0   \n",
      " previous_campaign_outcome  | 0   \n",
      " subscribed_deposit         | 0   \n",
      " created_at                 | 0   \n",
      " updated_at                 | 0   \n",
      " duration_in_year           | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store column expressions\n",
    "null_counts = []\n",
    "\n",
    "# Loop to iterate over each column in df_marketing\n",
    "for c in df_marketing.columns:\n",
    "    # Count the number of NULL values for column c\n",
    "    null_count_expr = F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "    \n",
    "    # Add the expression to the null_counts list\n",
    "    null_counts.append(null_count_expr)\n",
    "\n",
    "# Select the columns where the number of NULL values has been calculated\n",
    "df_null_counts = df_marketing.select(null_counts)\n",
    "\n",
    "# Display the result\n",
    "df_null_counts.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be693ae5-d89c-460b-9017-2fcca067689b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a600c0f0-c9de-49bd-95e4-2b043dc8018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " loan_data_id               | 0   \n",
      " age                        | 0   \n",
      " job                        | 0   \n",
      " marital_id                 | 0   \n",
      " education_id               | 0   \n",
      " default                    | 0   \n",
      " balance                    | 0   \n",
      " housing                    | 0   \n",
      " loan                       | 0   \n",
      " contact                    | 0   \n",
      " day                        | 0   \n",
      " month                      | 0   \n",
      " duration                   | 0   \n",
      " campaign                   | 0   \n",
      " days_since_last_campaign   | 0   \n",
      " previous_campaign_contacts | 0   \n",
      " previous_campaign_outcome  | 0   \n",
      " subscribed_deposit         | 0   \n",
      " created_at                 | 0   \n",
      " updated_at                 | 0   \n",
      " duration_in_year           | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_marketing.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_marketing.columns]).show(truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4277ec1-ef38-40c5-b0ce-8963ac126559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b255bcb6-74d7-4386-9c89-20176e7935b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------\n",
      " education_id | 0   \n",
      " value        | 0   \n",
      " created_at   | 0   \n",
      " updated_at   | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store column expressions\n",
    "null_counts = []\n",
    "\n",
    "# Loop to iterate over each column in df_education\n",
    "for c in df_education.columns:\n",
    "    # Count the number of NULL values for column c\n",
    "    null_count_expr = F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "    \n",
    "    # Add the expression to the null_counts list\n",
    "    null_counts.append(null_count_expr)\n",
    "\n",
    "# Select the columns where the number of NULL values has been calculated\n",
    "df_null_counts = df_education.select(null_counts)\n",
    "\n",
    "# Display the result\n",
    "df_null_counts.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e9a192-b3e3-465f-83e6-09ecad9f4ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f7e6bf2c-7964-497c-830e-09fd5c46b0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------\n",
      " education_id | 0   \n",
      " value        | 0   \n",
      " created_at   | 0   \n",
      " updated_at   | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_education.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_education.columns]).show(truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156d533-ecca-44bf-90cf-7ee1f5069901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "28fe654c-0c7c-4e65-b452-a4e785e00672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " marital_id | 0   \n",
      " value      | 0   \n",
      " created_at | 0   \n",
      " updated_at | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store column expressions\n",
    "null_counts = []\n",
    "\n",
    "# Loop to iterate over each column in df_marital\n",
    "for c in df_marital.columns:\n",
    "    # Count the number of NULL values for column c\n",
    "    null_count_expr = F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "    \n",
    "    # Add the expression to the null_counts list\n",
    "    null_counts.append(null_count_expr)\n",
    "\n",
    "# Select the columns where the number of NULL values has been calculated\n",
    "df_null_counts = df_marital.select(null_counts)\n",
    "\n",
    "# Display the result\n",
    "df_null_counts.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd3cccb-9418-4706-a5d9-526c85705fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d4fb892-ade5-42ec-b192-3bc57a6ae448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " marital_id | 0   \n",
      " value      | 0   \n",
      " created_at | 0   \n",
      " updated_at | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_marital.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_marital.columns]).show(truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a6905-2811-4085-b5a4-db4044708974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b2df4f5b-4b93-4229-a5e9-1074d9919d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------\n",
      " TransactionID           | 0    \n",
      " CustomerID              | 0    \n",
      " CustomerDOB             | 0    \n",
      " CustGender              | 1100 \n",
      " CustLocation            | 151  \n",
      " CustAccountBalance      | 2369 \n",
      " TransactionDate         | 0    \n",
      " TransactionTime         | 0    \n",
      " TransactionAmount (INR) | 0    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store column expressions\n",
    "null_counts = []\n",
    "\n",
    "# Loop to iterate over each column in df_transaction\n",
    "for c in df_transaction.columns:\n",
    "    # Count the number of NULL values for column c\n",
    "    null_count_expr = F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "    \n",
    "    # Add the expression to the null_counts list\n",
    "    null_counts.append(null_count_expr)\n",
    "\n",
    "# Select the columns where the number of NULL values has been calculated\n",
    "df_null_counts = df_transaction.select(null_counts)\n",
    "\n",
    "# Display the result\n",
    "df_null_counts.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2b67e-c13f-4895-b5c0-7af988c5de03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d7138287-9b74-4e9b-846d-5e615ea626ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TransactionID: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- CustomerDOB: string (nullable = true)\n",
      " |-- CustGender: string (nullable = true)\n",
      " |-- CustLocation: string (nullable = true)\n",
      " |-- CustAccountBalance: string (nullable = true)\n",
      " |-- TransactionDate: string (nullable = true)\n",
      " |-- TransactionTime: string (nullable = true)\n",
      " |-- TransactionAmount (INR): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transaction.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2367c-efd2-478e-8460-d998c5eb5f6d",
   "metadata": {},
   "source": [
    "### **2. Source to Target Mapping**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e7d91-5971-4acd-aba7-6f5f0b6e0fc9",
   "metadata": {},
   "source": [
    "\r\n",
    "## Column Mapping\r\n",
    "\r\n",
    "### Education Status\r\n",
    "source : table education_status\r\n",
    "\r\n",
    "target : table education_status\r\n",
    "\r\n",
    "| Source Column   | Target Column   | Transformation                                   |\r\n",
    "|----------------|----------------|---------------------------------------------------------|\r\n",
    "| `education_id` | `education_id` | -                       |\r\n",
    "| `value`        | `value`        | - |\r\n",
    "| `created_at`   | `created_at`   | -           |\r\n",
    "| `updated_at`   | `updated_at`   | -          |\r\n",
    "\r\n",
    "\r\n",
    "### Marital Status\r\n",
    "source : table marital_status\r\n",
    "\r\n",
    "target : table marital_status\r\n",
    "| Source Column   | Target Column   | Transformation |\r\n",
    "|----------------|----------------|---------------|\r\n",
    "| `marital_id`   | `marital_id`   | - |\r\n",
    "| `value`        | `value`        | - |\r\n",
    "| `created_at`   | `created_at`   | - |\r\n",
    "| `updated_at`   | `updated_at`   | - |\r\n",
    "\r\n",
    "\r\n",
    "### Marketing Campaign for Deposit\r\n",
    "source : table marketing_campaign_deposit\r\n",
    "\r\n",
    "target : table marketing_campaign_deposit\r\n",
    "| Source Column              | Target Column                | Transformation                                      |\r\n",
    "|----------------------------|-----------------------------|----------------------------------------------------|\r\n",
    "| `loan_data_id`             | `loan_data_id`              | - |\r\n",
    "| `age`                      | `age`                       | - |\r\n",
    "| `job`                      | `job`                       | - |\r\n",
    "| `marital_id`               | `marital_id`                | - |\r\n",
    "| `education_id`             | `education_id`              | - |\r\n",
    "| `\"default\"`                | `\"default\"`                 | - |\r\n",
    "| `balance`                  | `balance`                   | Remove `$` sign and convert to `INT` |\r\n",
    "| `housing`                  | `housing`                   | - |\r\n",
    "| `loan`                     | `loan`                      | - |\r\n",
    "| `contact`                  | `contact`                   | - |\r\n",
    "| `\"day\"`                    | `\"day\"`                     | - |\r\n",
    "| `\"month\"`                  | `\"month\"`                   | - |\r\n",
    "| `duration`                 | `duration`                  | - |\r\n",
    "| `duration`                 | `duration_in_year`          | duration divide by `365`, round down, and cast to `INT` |\r\n",
    "| `campaign`                 | `campaign`                  | - |\r\n",
    "| `pdays`                    | `days_since_last_campaign`  | Rename column |\r\n",
    "| `previous`                 | `previous_campaign_contacts`| Rename column |\r\n",
    "| `poutcome`                 | `previous_campaign_outcome` | Rename column |\r\n",
    "| `subscribed_deposit`       | `subscribed_deposit`        | - |\r\n",
    "| `created_at`               | `created_at`                | - |\r\n",
    "| `updated_at`               | `updated_at`                | - |\r\n",
    "\r\n",
    "### Customers\r\n",
    "source : file new_bank_transaction.csv\r\n",
    "\r\n",
    "target : table customers\r\n",
    "\r\n",
    "| Source Column          | Target Column      | Transformation                                      |\r\n",
    "|------------------------|-------------------|----------------------------------------------------|\r\n",
    "| `CustomerID`          | `customer_id`      | Rename column |\r\n",
    "| `CustomerDOB`         | `birth_date`       | Convert to `DATE` format (`d/M/yy`), adjust years if > 2025 |\r\n",
    "| `CustGender`          | `gender`           | Rename column; Map `M` → `Male`, `F` → `Female`, others → `Other` |\r\n",
    "| `CustLocation`        | `location`         | Rename column |\r\n",
    "| `CustAccountBalance`  | `account_balance`  | Rename column, cast to decimal number |\r\n",
    "\r\n",
    "### Transactions\r\n",
    "source : file new_bank_transaction.csv\r\n",
    "\r\n",
    "target : table transactions\r\n",
    "\r\n",
    "| Source Column                 | Target Column      | Transformation                                                   |\r\n",
    "|--------------------------------|-------------------|-----------------------------------------------------------------|\r\n",
    "| `TransactionID`               | `transaction_id`  | Rename column |\r\n",
    "| `CustomerID`                  | `customer_id`     | Rename column |\r\n",
    "| `TransactionDate`             | `transaction_date` | Convert to `DATE` format (`d/M/yy`), adjust years if > 2025 |\r\n",
    "| `TransactionTime`             | `transaction_time` | Convert to `HH:MM:SS` format |\r\n",
    "| `TransactionAmount (INR)`     | `transaction_amount` | Rename column, cast to decimal number |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb1a21b-2e69-46cc-ba32-6e8050e1a9c2",
   "metadata": {},
   "source": [
    "### **3. Code Testing**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dee1cf-d0d8-40cd-8029-9c51f9a734c4",
   "metadata": {},
   "source": [
    "## helper.py (logging & init spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2cfa50a1-0dbf-4c72-9752-d6ea1a9cd1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def logging_process(log_file=\"script/log/info.log\"):\n",
    "    # Configure logging\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=log_file,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "    return logger\n",
    "\n",
    "\n",
    "def init_spark_session():\n",
    "    spark = SparkSession.builder.appName(\n",
    "        \"Exercise Data Pipeline Week_6\"\n",
    "    ).getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2523d6b-b7e8-4eb7-a0de-7d8537263d87",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee00920-dd37-4472-a88f-42681d48287b",
   "metadata": {},
   "source": [
    "## extract_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3cee24ee-2601-4ca7-9084-60dc0f1c3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pyspark\n",
    "\n",
    "logging_process()\n",
    "\n",
    "\n",
    "def extract_data(\n",
    "    data_name: str, format_data: str\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to extract movie data in csv or database table\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_name (str): name of data or table of data sources\n",
    "    format_data (str): format data of data sources, currently on csv or db\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df (pyspark.sql.DataFrame): dataframe of data sources\n",
    "    \"\"\"\n",
    "    # create spark session\n",
    "    spark = init_spark_session()\n",
    "\n",
    "    # set variable for database\n",
    "    DB_URL = \"jdbc:postgresql://source_db:5432/source\"\n",
    "    DB_USER = \"postgres\"\n",
    "    DB_PASS = \"postgres\"\n",
    "\n",
    "    # set config\n",
    "    jdbc_url = DB_URL\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if format_data.lower() == \"csv\":\n",
    "            logging.info(f\"===== Start Extracting {data_name} data =====\")\n",
    "\n",
    "            df = spark.read.csv(f\"data/{data_name}.csv\", header=True)\n",
    "\n",
    "            logging.info(f\"===== Finish Extracting {data_name} data =====\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        elif format_data.lower() == \"db\":\n",
    "            logging.info(f\"===== Start Extracting {data_name} data =====\")\n",
    "\n",
    "            df = spark.read.jdbc(\n",
    "                url=jdbc_url, table=data_name, properties=connection_properties\n",
    "            )\n",
    "\n",
    "            logging.info(f\"===== Finish Extracting {data_name} data =====\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Format data not supported yet\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"====== Failed to Extract Data ======\")\n",
    "        logging.error(e)\n",
    "\n",
    "        raise Exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cb4d41-655e-433d-8905-93e9ba7151a3",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680c61b-6a15-42a2-a9e4-c11646e53f90",
   "metadata": {},
   "source": [
    "## transform.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a3066-9542-4d61-98b4-99c8fbff8bea",
   "metadata": {},
   "source": [
    "### a. convert_date.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "ba6f1d16-b59f-43ed-b255-5e5d6e42abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_date, col, when, concat, substring, lit, lpad, regexp_extract\n",
    "\n",
    "def convert_date_columns(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to convert date and time columns based on table name.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        table_name: Name of the table being processed\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with converted date and time columns\n",
    "    \"\"\"\n",
    "    if table_name == \"transactions\":\n",
    "        # Convert TransactionDate from d/M/yy to YYYY/MM/DD\n",
    "        df = df.withColumn(\"transaction_date\", to_date(col(\"transaction_date\"), \"d/M/yy\"))\n",
    "        \n",
    "        # Convert TransactionTime from HHMMSS to HH:MM:SS format using lpad\n",
    "        df = df.withColumn(\"padded_time\", lpad(col(\"transaction_time\"), 6, \"0\"))\n",
    "        \n",
    "        # Convert to HH:MM:SS format\n",
    "        df = df.withColumn(\n",
    "            \"transaction_time\",\n",
    "            concat(\n",
    "                substring(col(\"padded_time\"), 1, 2), lit(\":\"),\n",
    "                substring(col(\"padded_time\"), 3, 2), lit(\":\"),\n",
    "                substring(col(\"padded_time\"), 5, 2)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Drop the temporary column\n",
    "        df = df.drop(\"padded_time\")\n",
    "        \n",
    "    elif table_name == \"customers\":\n",
    "        # Convert CustomerDOB with year > 25 check using regex for safer extraction\n",
    "        df = df.withColumn(\n",
    "            \"birth_date\",\n",
    "            when(\n",
    "                regexp_extract(col(\"birth_date\"), \"(\\\\d{1,2})/(\\\\d{1,2})/(\\\\d{2})\", 3).cast(\"int\") > 25,\n",
    "                to_date(\n",
    "                    concat(\n",
    "                        regexp_extract(col(\"birth_date\"), \"(\\\\d{1,2})/(\\\\d{1,2})/\", 1), \n",
    "                        lit(\"/\"),\n",
    "                        regexp_extract(col(\"birth_date\"), \"\\\\d{1,2}/(\\\\d{1,2})/\", 1),\n",
    "                        lit(\"/19\"),\n",
    "                        regexp_extract(col(\"birth_date\"), \"\\\\d{1,2}/\\\\d{1,2}/(\\\\d{2})\", 1)\n",
    "                    ),\n",
    "                    \"d/M/yyyy\"\n",
    "                )\n",
    "            ).otherwise(to_date(col(\"birth_date\"), \"d/M/yy\"))\n",
    "        )\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a04eade-0abc-4d4e-90d9-efc3bc599fbe",
   "metadata": {},
   "source": [
    "### b. casting_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "8f21f640-04c2-4060-b1e2-e501ae1971e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, round, regexp_replace\n",
    "\n",
    "def casting_data_types(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to cast data types based on table name.\n",
    "    \"\"\"\n",
    "    casting_mappings = {\n",
    "        \"marketing_campaign_deposit\": {\n",
    "            \"balance\": (\"int\", \"\\\\$\"),\n",
    "            \"duration_in_year\": (\"int\", None, \"duration\", 365)\n",
    "        },\n",
    "        \"transactions\": {\n",
    "            \"transaction_amount\": \"double\"\n",
    "        },\n",
    "        \"customers\": {\n",
    "            \"account_balance\": \"double\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if table_name in casting_mappings:\n",
    "        for col_name, cast_info in casting_mappings[table_name].items():\n",
    "            if isinstance(cast_info, tuple):\n",
    "                if len(cast_info) == 2 and cast_info[1]:\n",
    "                    df = df.withColumn(col_name, regexp_replace(col(col_name), cast_info[1], \"\").cast(cast_info[0]))\n",
    "                elif len(cast_info) == 4:\n",
    "                    df = df.withColumn(col_name, round(col(cast_info[2]) / cast_info[3]).cast(cast_info[0]))\n",
    "            else:\n",
    "                df = df.withColumn(col_name, col(col_name).cast(cast_info))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301e798-51ef-441e-8ed5-fd6e6039331e",
   "metadata": {},
   "source": [
    "### c. select_column.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "68a26e8c-23c7-4837-b81d-f842f3c49078",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import pyspark\n",
    "\n",
    "logging_process()\n",
    "\n",
    "\n",
    "def select_columns_process(\n",
    "    df_result: pyspark.sql.DataFrame, table_name: str\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Function that selects columns based on the table name from the list of columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_result (pyspark.sql.DataFrame): Input DataFrame for the specific table.\n",
    "    table_name (str): The name of the table used to select the appropriate columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark.sql.DataFrame: DataFrame with selected columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"===== Start Selecting Data process for table {table_name} =====\")\n",
    "\n",
    "        # Define columns for each table\n",
    "        table_columns = {\n",
    "            \"marital_status\": [\"marital_id\", \"value\"],\n",
    "            \"education_status\": [\"education_id\", \"value\"],\n",
    "            \"marketing_campaign_deposit\": [\n",
    "                \"loan_data_id\", \"age\", \"job\", \"marital_id\", \"education_id\", \"default\", \"balance\",\n",
    "                \"housing\", \"loan\", \"contact\", \"day\", \"month\", \"duration\", \"duration_in_year\", \n",
    "                \"campaign\", \"days_since_last_campaign\", \"previous_campaign_contacts\", \n",
    "                \"previous_campaign_outcome\", \"subscribed_deposit\"\n",
    "            ],\n",
    "            \"customers\": [\n",
    "                \"customer_id\", \"birth_date\", \"gender\", \"location\", \"account_balance\"\n",
    "            ],\n",
    "            \"transactions\": [\n",
    "                \"transaction_id\", \"customer_id\", \"transaction_date\", \"transaction_time\", \n",
    "                \"transaction_amount\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Check if the table_name is in the dictionary and select the columns\n",
    "        if table_name in table_columns:\n",
    "            selected_cols = table_columns[table_name]\n",
    "            df_result = df_result.select(*selected_cols)\n",
    "        else:\n",
    "            raise ValueError(f\"Table name '{table_name}' not recognized!\")\n",
    "\n",
    "        logging.info(f\"===== Finish Selecting Data process for table {table_name} =====\")\n",
    "\n",
    "        return df_result\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"===== Failed Selecting Data process for table {table_name} =====\")\n",
    "        logging.error(e)\n",
    "        raise Exception(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273c508a-e8a0-4da8-9b3e-85e8bc509db8",
   "metadata": {},
   "source": [
    "### d. clean_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "2092e322-8c6d-4ec9-99c1-3c9c0975bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, trim, lower, when\n",
    "\n",
    "def clean_data(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to clean data before processing.\n",
    "    \"\"\"\n",
    "    if table_name == \"customers\":\n",
    "        df = df.withColumn(\"gender\", lower(trim(col(\"gender\"))))\n",
    "        df = df.withColumn(\"gender\", when(col(\"gender\") == \"m\", \"Male\")\n",
    "                           .when(col(\"gender\") == \"f\", \"Female\")\n",
    "                           .otherwise(\"Other\"))\n",
    "       \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47817a-01cc-491a-b684-7b478313444a",
   "metadata": {},
   "source": [
    "### e. rename_column.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "8238399d-fe99-42a6-a213-0d07e94348d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def rename_columns(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to rename columns based on standardized naming convention.\n",
    "    \"\"\"\n",
    "    rename_mappings = {\n",
    "        \"marketing_campaign_deposit\": {\n",
    "            \"pdays\": \"days_since_last_campaign\",\n",
    "            \"previous\": \"previous_campaign_contacts\",\n",
    "            \"poutcome\": \"previous_campaign_outcome\"\n",
    "        },        \n",
    "        \"transactions\": {\n",
    "            \"TransactionID\": \"transaction_id\",\n",
    "            \"CustomerID\": \"customer_id\",\n",
    "            \"TransactionDate\": \"transaction_date\",\n",
    "            \"TransactionTime\": \"transaction_time\",\n",
    "            \"TransactionAmount (INR)\": \"transaction_amount\"\n",
    "        },\n",
    "        \"customers\": {\n",
    "            \"CustomerID\": \"customer_id\",\n",
    "            \"CustomerDOB\": \"birth_date\",\n",
    "            \"CustGender\": \"gender\",\n",
    "            \"CustLocation\": \"location\",\n",
    "            \"CustAccountBalance\": \"account_balance\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if table_name in rename_mappings:\n",
    "        for old_col, new_col in rename_mappings[table_name].items():\n",
    "            df = df.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6444a-7f51-4f8b-a810-b4097928ab7a",
   "metadata": {},
   "source": [
    "### transform_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "38326822-6672-49ca-b84f-f05249ead5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "logging_process()\n",
    "\n",
    "def transform_data(df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to apply all transformation steps on the dataframe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"===== Start Transforming Data for {table_name} =====\")\n",
    "        df = rename_columns(df, table_name)\n",
    "        df = convert_date_columns(df, table_name)\n",
    "        df = casting_data_types(df, table_name)\n",
    "        df = select_columns_process(df, table_name)\n",
    "        df = clean_data(df, table_name)\n",
    "        logging.info(f\"===== Finished Transforming Data for {table_name} =====\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"===== Failed to Transform Data for {table_name} =====\")\n",
    "        logging.error(e)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1310f5a-4bf6-4ea7-bc22-d6e0793bd595",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374983a-e889-4493-b1a2-3f9a6e7fca89",
   "metadata": {},
   "source": [
    "## load_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "0176ddf4-0b95-4fb2-8e28-39d77a40f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import psycopg2\n",
    "from helper.utils import logging_process\n",
    "import pyspark\n",
    "\n",
    "logging_process()\n",
    "\n",
    "def load_data(df_result: pyspark.sql.DataFrame, table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Function that dumps the result to the database using PySpark\n",
    "    and maintains data integrity by truncating the table before loading new data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_result (pyspark.sql.DataFrame): Final result of pyspark dataframe.\n",
    "    table_name (str): The target table name in the database where data needs to be loaded.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set variable for database\n",
    "        DB_URL = \"jdbc:postgresql://data_warehouse:5432/data_warehouse\"\n",
    "        DB_USER = \"postgres\"\n",
    "        DB_PASS = \"postgres\"\n",
    "        JDBC_URL = DB_URL\n",
    "        connection_properties = {\n",
    "            \"user\": DB_USER,\n",
    "            \"password\": DB_PASS,\n",
    "            \"driver\": \"org.postgresql.Driver\"  # Set driver postgres\n",
    "        }\n",
    "\n",
    "        logging.info(\"===== Start Load Data to the Database =====\")\n",
    "\n",
    "        # Step 1: Truncate the target table (ensure data integrity by removing old records before loading new data)\n",
    "        # Connect to PostgreSQL using psycopg2 for executing non-query SQL (such as TRUNCATE)\n",
    "        with psycopg2.connect(\n",
    "            host=\"data_warehouse\", dbname=\"data_warehouse\", user=DB_USER, password=DB_PASS\n",
    "        ) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                truncate_sql = f\"TRUNCATE TABLE {table_name} CASCADE\"\n",
    "                cursor.execute(truncate_sql)\n",
    "                conn.commit()  # Ensure changes are committed\n",
    "                logging.info(f\"===== Truncated table {table_name} successfully =====\")\n",
    "\n",
    "        # Step 2: Load new data using the 'append' method\n",
    "        df_result.write.jdbc(\n",
    "            url=JDBC_URL,\n",
    "            table=table_name,\n",
    "            mode=\"append\",  # Use append to add data to the table without deleting existing data\n",
    "            properties=connection_properties,\n",
    "        )\n",
    "\n",
    "        logging.info(\"===== Finished Load Data to the Database =====\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"===== Failed Load Data to the Database for table {table_name} =====\")\n",
    "        logging.error(e)\n",
    "        raise Exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82323c-e37a-49c0-97e8-ab881f7cb725",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f0524-1468-4407-ba90-451d8cfd2a1f",
   "metadata": {},
   "source": [
    "## run_pyspark_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "b3bc8020-feda-4f27-aaee-9c3f000ec6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "# Initialize logging\n",
    "logging_process()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"===== Start Banking Data Pipeline =====\")\n",
    "\n",
    "    try:\n",
    "        # Extract data from CSV and database\n",
    "        df_transactions = extract_data(data_name=\"new_bank_transaction\", format_data=\"csv\")\n",
    "        df_customers = extract_data(data_name=\"new_bank_transaction\", format_data=\"csv\")\n",
    "        df_marketing = extract_data(data_name=\"marketing_campaign_deposit\", format_data=\"db\")\n",
    "        df_education = extract_data(data_name=\"education_status\", format_data=\"db\")\n",
    "        df_marital = extract_data(data_name=\"marital_status\", format_data=\"db\")\n",
    "\n",
    "        # Transform each dataset separately\n",
    "        df_transactions = transform_data(df_transactions, \"transactions\")\n",
    "        df_customers = transform_data(df_customers, \"customers\")\n",
    "        df_marketing = transform_data(df_marketing, \"marketing_campaign_deposit\")\n",
    "        df_education = transform_data(df_education, \"education_status\")\n",
    "        df_marital = transform_data(df_marital, \"marital_status\")\n",
    "\n",
    "        # Load each transformed dataset into the data warehouse\n",
    "        load_data(df_education, table_name=\"education_status\")\n",
    "        load_data(df_marital, table_name=\"marital_status\")\n",
    "        load_data(df_customers, table_name=\"customers\")\n",
    "        load_data(df_transactions, table_name=\"transactions\")\n",
    "        load_data(df_marketing, table_name=\"marketing_campaign_deposit\")\n",
    "\n",
    "        logging.info(\"===== Finish Banking Data Pipeline =====\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"===== Data Pipeline Failed =====\")\n",
    "        logging.error(e)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd4ea9a-db1d-4333-a0b8-e7ac324d38bc",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b408d0-184b-4b3c-85d7-c9ab52e862ad",
   "metadata": {},
   "source": [
    "## Output Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "f544afaa-2f2a-4327-b079-056f37a54e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = extract_data(data_name=\"new_bank_transaction\", format_data=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "8074b363-7840-4ef5-a409-861e17aad2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = transform_data(df_customers, \"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "f5605f40-4898-4704-a14b-dad62021be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(df_customers, table_name=\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "77487c23-ab28-43e9-a0b7-08b79c9fdd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------\n",
      " customer_id     | C1010028   \n",
      " birth_date      | 1988-08-25 \n",
      " gender          | Female     \n",
      " location        | DELHI      \n",
      " account_balance | 296828.37  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.show(1, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "35a6d89f-c968-47d0-a84c-a1b9eaba5b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Birth Date: 1800-01-01\n",
      "Maximum Birth Date: 2025-05-06\n"
     ]
    }
   ],
   "source": [
    "min_bod_date = df_customers.select(\"birth_date\").agg({\"birth_date\": \"min\"}).collect()[0][0]\n",
    "max_bod_date = df_customers.select(\"birth_date\").agg({\"birth_date\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(f\"Minimum Birth Date: {min_bod_date}\")\n",
    "print(f\"Maximum Birth Date: {max_bod_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "fa069036-1793-404b-a8fd-b1dc6d38b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|Female|\n",
      "|Other |\n",
      "|Male  |\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.select(\"gender\").distinct().show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9333205d-0ca6-4685-9eb2-6b238636cdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "818080e3-9b7f-4c25-ac20-bbff1a40555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------\n",
      " transaction_id     | T642232    \n",
      " customer_id        | C1010028   \n",
      " transaction_date   | 2016-08-29 \n",
      " transaction_time   | 09:52:12   \n",
      " transaction_amount | 557.0      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show(1, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "3c677796-719e-451a-ae99-bef1843f2663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Transaction Date: 2016-08-01\n",
      "Maximum Transaction Date: 2016-10-21\n"
     ]
    }
   ],
   "source": [
    "min_tr_date = df_transactions.select(\"transaction_date\").agg({\"transaction_date\": \"min\"}).collect()[0][0]\n",
    "max_tr_date = df_transactions.select(\"transaction_date\").agg({\"transaction_date\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(f\"Minimum Transaction Date: {min_tr_date}\")\n",
    "print(f\"Maximum Transaction Date: {max_tr_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "1f72a062-6bc9-45aa-8b77-89f9a0291a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Transaction Time: 00:00:00\n",
      "Maximum Transaction Time: 23:59:59\n"
     ]
    }
   ],
   "source": [
    "min_tr_time = df_transactions.select(\"transaction_time\").agg({\"transaction_time\": \"min\"}).collect()[0][0]\n",
    "max_tr_time = df_transactions.select(\"transaction_time\").agg({\"transaction_time\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(f\"Minimum Transaction Time: {min_tr_time}\")\n",
    "print(f\"Maximum Transaction Time: {max_tr_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "c362b383-92dc-4416-aef3-7c30a015e435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Transaction Amount: 0.0\n",
      "Maximum Transaction Amount: 1560034.99\n"
     ]
    }
   ],
   "source": [
    "min_tr_amount = df_transactions.select(\"transaction_amount\").agg({\"transaction_amount\": \"min\"}).collect()[0][0]\n",
    "max_tr_amount = df_transactions.select(\"transaction_amount\").agg({\"transaction_amount\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(f\"Minimum Transaction Amount: {min_tr_amount}\")\n",
    "print(f\"Maximum Transaction Amount: {max_tr_amount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed36547-a21a-47cc-bc97-d62c88723bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "e2c67c43-bffc-494e-a795-8a9e33b67f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " loan_data_id               | 1          \n",
      " age                        | 58         \n",
      " job                        | management \n",
      " marital_id                 | 1          \n",
      " education_id               | 1          \n",
      " default                    | false      \n",
      " balance                    | 2143       \n",
      " housing                    | true       \n",
      " loan                       | false      \n",
      " contact                    | unknown    \n",
      " day                        | 5          \n",
      " month                      | may        \n",
      " duration                   | 261        \n",
      " duration_in_year           | 1          \n",
      " campaign                   | 1          \n",
      " days_since_last_campaign   | -1         \n",
      " previous_campaign_contacts | 0          \n",
      " previous_campaign_outcome  | unknown    \n",
      " subscribed_deposit         | false      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_marketing.show(1, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "4677ee4b-b943-44ee-af0a-88499c1a14ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|previous_campaign_outcome|\n",
      "+-------------------------+\n",
      "|success                  |\n",
      "|unknown                  |\n",
      "|other                    |\n",
      "|failure                  |\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_marketing.select(\"previous_campaign_outcome\").distinct().show(truncate = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
